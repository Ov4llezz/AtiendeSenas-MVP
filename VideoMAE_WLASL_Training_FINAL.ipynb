{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ü VideoMAE WLASL Training - Sistema Completo\n",
    "\n",
    "**Autor:** Rafael Ovalle - Tesis UNAB  \n",
    "**Dataset:** WLASL100/WLASL300 (V1 y V2)  \n",
    "**Modelo:** VideoMAE (MCG-NJU/videomae-base-finetuned-kinetics)  \n",
    "\n",
    "---\n",
    "\n",
    "## üìã Estructura del Notebook\n",
    "\n",
    "1. ‚úÖ **Configuraci√≥n Inicial** - Verificar GPU y entorno\n",
    "2. üì¶ **Clonar Repositorio y Dataset** - Desde GitHub y Google Drive\n",
    "3. üîç **Verificar Dataset** - Estructura y splits\n",
    "4. ‚öôÔ∏è **Configurar Hiperpar√°metros** - Elegir dataset y versi√≥n\n",
    "5. üöÄ **Entrenamiento** - Largo o corto de prueba\n",
    "6. üìä **Evaluaci√≥n** - Listar modelos y evaluar\n",
    "7. üìà **Visualizaci√≥n** - M√©tricas y gr√°ficos\n",
    "8. üíæ **Descargar Resultados** - Comprimir y exportar\n",
    "9. üõ†Ô∏è **Utilidades** - Monitorear GPU y limpiar memoria\n",
    "\n",
    "---\n",
    "\n",
    "**IMPORTANTE:**  \n",
    "- Este notebook est√° dise√±ado para **Google Colab conectado a VM local**\n",
    "- Usa `!python3` para comandos en la VM\n",
    "- Dataset clonado con `shutil` desde Google Drive\n",
    "- Ruta dataset: `/home/ov4lle/AtiendeSenas-MVP/data/`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1Ô∏è‚É£ Configuraci√≥n Inicial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Verificar GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar GPU con nvidia-smi\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar GPU en PyTorch\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU no disponible - entrenamiento ser√° en CPU (muy lento)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Verificar Uso de Disco\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar espacio en disco\n",
    "!df -h | grep -E \"Filesystem|/home\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2Ô∏è‚É£ Clonar Repositorio y Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Clonar Repositorio desde GitHub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clonar repositorio (solo si no existe)\n",
    "import os\n",
    "\n",
    "REPO_PATH = \"/home/ov4lle/AtiendeSenas-MVP\"\n",
    "\n",
    "if not os.path.exists(REPO_PATH):\n",
    "    print(\"üì¶ Clonando repositorio desde GitHub...\")\n",
    "    !git clone https://github.com/Ov4llezz/AtiendeSenas-MVP.git {REPO_PATH}\n",
    "else:\n",
    "    print(\"‚úÖ Repositorio ya existe\")\n",
    "    print(\"üîÑ Actualizando desde GitHub...\")\n",
    "    !cd {REPO_PATH} && git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Montar Google Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Montar Google Drive (Colab)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Clonar Dataset desde Google Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar rutas\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Ruta en Google Drive (AJUSTAR seg√∫n tu estructura)\n",
    "DRIVE_DATASET_PATH = \"/content/drive/MyDrive/TESIS/datasets\"  # CAMBIAR ESTO\n",
    "\n",
    "# Ruta destino en VM\n",
    "VM_DATA_PATH = \"/home/ov4lle/AtiendeSenas-MVP/data\"\n",
    "\n",
    "# Crear directorio si no existe\n",
    "os.makedirs(VM_DATA_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"üîç Buscando datasets en: {DRIVE_DATASET_PATH}\")\n",
    "print(f\"üìÇ Destino: {VM_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de datasets a copiar\n",
    "DATASETS = [\"wlasl100\", \"wlasl300\", \"wlasl100_v2\", \"wlasl300_v2\"]\n",
    "\n",
    "for dataset_name in DATASETS:\n",
    "    src = os.path.join(DRIVE_DATASET_PATH, dataset_name)\n",
    "    dst = os.path.join(VM_DATA_PATH, dataset_name)\n",
    "    \n",
    "    if os.path.exists(src):\n",
    "        if not os.path.exists(dst):\n",
    "            print(f\"\\nüì¶ Copiando {dataset_name}...\")\n",
    "            shutil.copytree(src, dst)\n",
    "            print(f\"‚úÖ {dataset_name} copiado\")\n",
    "        else:\n",
    "            print(f\"‚úÖ {dataset_name} ya existe en VM\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è {dataset_name} no encontrado en Drive\")\n",
    "\n",
    "print(\"\\n‚úÖ Proceso de copia finalizado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3Ô∏è‚É£ Verificar Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiar al directorio del proyecto\n",
    "%cd /home/ov4lle/AtiendeSenas-MVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar estructura de los 4 datasets\n",
    "from colab_utils.dataset_utils import verify_all_datasets\n",
    "\n",
    "verify_all_datasets(data_root=\"/home/ov4lle/AtiendeSenas-MVP/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4Ô∏è‚É£ Configurar Hiperpar√°metros y Elegir Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Configuraci√≥n de Hiperpar√°metros\n",
    "\n",
    "**Par√°metros disponibles:**\n",
    "- `dataset_type`: \"wlasl100\" o \"wlasl300\"\n",
    "- `version`: \"v1\" (train/val/test separados) o \"v2\" (train+val combinados)\n",
    "- `batch_size`: Tama√±o del batch (default: 6)\n",
    "- `max_epochs`: N√∫mero m√°ximo de epochs (default: 30)\n",
    "- `learning_rate`: Learning rate inicial (default: 1e-5)\n",
    "- `patience`: Epochs sin mejora para early stopping (default: 10)\n",
    "- `weight_decay`: Regularizaci√≥n L2 (default: 0.0)\n",
    "- `label_smoothing`: Label smoothing (default: 0.0)\n",
    "- `class_weighted`: Usar pesos de clases (default: False)\n",
    "- `freeze_backbone`: Congelar backbone VideoMAE (default: False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_utils.config import create_config, print_config\n",
    "\n",
    "# ============================================================\n",
    "#   CONFIGURACI√ìN DEL EXPERIMENTO\n",
    "# ============================================================\n",
    "\n",
    "config = create_config(\n",
    "    # Dataset y versi√≥n\n",
    "    dataset_type=\"wlasl100\",      # \"wlasl100\" o \"wlasl300\"\n",
    "    version=\"v2\",                  # \"v1\" o \"v2\"\n",
    "    data_root=\"/home/ov4lle/AtiendeSenas-MVP/data\",\n",
    "    \n",
    "    # Hiperpar√°metros principales\n",
    "    batch_size=6,\n",
    "    max_epochs=30,\n",
    "    learning_rate=1e-5,\n",
    "    patience=10,\n",
    "    \n",
    "    # Regularizaci√≥n\n",
    "    weight_decay=0.0,\n",
    "    label_smoothing=0.0,\n",
    "    class_weighted=False,\n",
    "    freeze_backbone=False,         # False = entrenar todas las capas\n",
    "    \n",
    "    # Otros\n",
    "    warmup_ratio=0.1,\n",
    "    min_lr=1e-6,\n",
    "    gradient_clip=1.0,\n",
    "    num_workers=2,\n",
    "    save_every=5\n",
    ")\n",
    "\n",
    "# Mostrar configuraci√≥n\n",
    "print_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5Ô∏è‚É£ Entrenamiento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Entrenamiento CORTO (Prueba - 3 epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento corto de prueba (3 epochs)\n",
    "from colab_utils.training import train_model\n",
    "from colab_utils.dataset import create_dataloaders\n",
    "\n",
    "# Crear dataloaders\n",
    "train_loader, val_loader, train_dataset = create_dataloaders(config)\n",
    "\n",
    "# Configuraci√≥n temporal para prueba r√°pida\n",
    "config_prueba = config.copy()\n",
    "config_prueba['max_epochs'] = 3\n",
    "config_prueba['save_every'] = 1\n",
    "\n",
    "print(\"\\nüöÄ Iniciando entrenamiento de PRUEBA (3 epochs)...\\n\")\n",
    "\n",
    "# Entrenar\n",
    "model, history, checkpoint_dir, log_dir = train_model(\n",
    "    config_prueba,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    train_dataset\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Entrenamiento de prueba completado\")\n",
    "print(f\"üìÅ Checkpoints: {checkpoint_dir}\")\n",
    "print(f\"üìä Logs: {log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Entrenamiento COMPLETO (30 epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento completo (30 epochs o lo configurado)\n",
    "from colab_utils.training import train_model\n",
    "from colab_utils.dataset import create_dataloaders\n",
    "\n",
    "# Crear dataloaders\n",
    "train_loader, val_loader, train_dataset = create_dataloaders(config)\n",
    "\n",
    "print(f\"\\nüöÄ Iniciando entrenamiento COMPLETO ({config['max_epochs']} epochs)...\\n\")\n",
    "\n",
    "# Entrenar\n",
    "model, history, checkpoint_dir, log_dir = train_model(\n",
    "    config,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    train_dataset\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Entrenamiento completado\")\n",
    "print(f\"üìÅ Checkpoints: {checkpoint_dir}\")\n",
    "print(f\"üìä Logs: {log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6Ô∏è‚É£ Evaluaci√≥n de Modelo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Listar Modelos Disponibles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar modelos V1 (wlasl100/300 originales)\n",
    "!python3 scripts/test.py --list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar modelos V2 (wlasl100_v2/300_v2 - train+val combinados)\n",
    "!python3 scripts_v2/test.py --list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Evaluar Modelo por ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar modelo V1 por ID\n",
    "# Ejemplo: si el ID es 1, eval√∫a el run #1\n",
    "\n",
    "MODEL_ID_V1 = 1  # CAMBIAR seg√∫n el listado\n",
    "\n",
    "!python3 scripts/test.py --run-id {MODEL_ID_V1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar modelo V2 por ID\n",
    "MODEL_ID_V2 = 1  # CAMBIAR seg√∫n el listado\n",
    "\n",
    "!python3 scripts_v2/test.py --run-id {MODEL_ID_V2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Evaluar Modelo por Path Espec√≠fico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar modelo V1 por path espec√≠fico\n",
    "CHECKPOINT_PATH_V1 = \"models/checkpoints/run_20251130_123456/best_model.pt\"  # CAMBIAR\n",
    "\n",
    "!python3 scripts/test.py --checkpoint {CHECKPOINT_PATH_V1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar modelo V2 por path espec√≠fico\n",
    "CHECKPOINT_PATH_V2 = \"models_v2/checkpoints/run_20251130_123456/best_model.pt\"  # CAMBIAR\n",
    "\n",
    "!python3 scripts_v2/test.py --checkpoint {CHECKPOINT_PATH_V2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7Ô∏è‚É£ Visualizaci√≥n de Resultados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Matriz de Confusi√≥n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import glob\n",
    "\n",
    "# Buscar la matriz de confusi√≥n m√°s reciente\n",
    "confusion_matrices = sorted(glob.glob(\"evaluation_results*/confusion_matrix.png\"), reverse=True)\n",
    "\n",
    "if confusion_matrices:\n",
    "    print(f\"üìä Mostrando: {confusion_matrices[0]}\")\n",
    "    display(Image(filename=confusion_matrices[0]))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se encontraron matrices de confusi√≥n. Ejecuta la evaluaci√≥n primero.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 M√©tricas Detalladas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Buscar el reporte JSON m√°s reciente\n",
    "json_reports = sorted(glob.glob(\"evaluation_results*/test_results_*.json\"), reverse=True)\n",
    "\n",
    "if json_reports:\n",
    "    with open(json_reports[0], 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'RESULTADOS DE EVALUACI√ìN':^70}\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Accuracy Total: {results['overall_accuracy']:.2f}%\")\n",
    "    print(f\"Top-3 Accuracy: {results.get('top3_accuracy', 'N/A')}\")\n",
    "    print(f\"Top-5 Accuracy: {results.get('top5_accuracy', 'N/A')}\")\n",
    "    print(f\"\\nPrecision (macro): {results.get('precision_macro', 'N/A')}\")\n",
    "    print(f\"Recall (macro): {results.get('recall_macro', 'N/A')}\")\n",
    "    print(f\"F1-score (macro): {results.get('f1_macro', 'N/A')}\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se encontraron reportes JSON. Ejecuta la evaluaci√≥n primero.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Curvas de Entrenamiento (TensorBoard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar TensorBoard\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Mostrar logs del √∫ltimo run\n",
    "# CAMBIAR seg√∫n el directorio del √∫ltimo run\n",
    "LOG_DIR = \"runs/v2/wlasl100_v2\"  # o \"runs_v2/...\"\n",
    "\n",
    "%tensorboard --logdir {LOG_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 8Ô∏è‚É£ Descargar Resultados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Comprimir y Descargar Todo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# Crear archivo zip con todos los resultados\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_zip = f\"resultados_completos_{timestamp}\"\n",
    "\n",
    "# Agregar al zip: checkpoints, logs, evaluation_results\n",
    "!zip -r {output_zip}.zip models/ models_v2/ runs/ runs_v2/ evaluation_results/ evaluation_results_v2/\n",
    "\n",
    "print(f\"\\n‚úÖ Archivo creado: {output_zip}.zip\")\n",
    "print(f\"üì¶ Tama√±o: \", end=\"\")\n",
    "!du -h {output_zip}.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar archivo zip a tu m√°quina local\n",
    "from google.colab import files\n",
    "\n",
    "files.download(f\"{output_zip}.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Descargar Archivos Individuales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar matriz de confusi√≥n\n",
    "if confusion_matrices:\n",
    "    files.download(confusion_matrices[0])\n",
    "\n",
    "# Descargar reporte JSON\n",
    "if json_reports:\n",
    "    files.download(json_reports[0])\n",
    "\n",
    "# Descargar mejor modelo\n",
    "best_models = sorted(glob.glob(\"models*/checkpoints/*/best_model.pt\"), reverse=True)\n",
    "if best_models:\n",
    "    files.download(best_models[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Copiar a Google Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiar resultados a Google Drive\n",
    "DRIVE_OUTPUT_PATH = \"/content/drive/MyDrive/TESIS/resultados\"  # CAMBIAR ESTO\n",
    "\n",
    "os.makedirs(DRIVE_OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Copiar zip\n",
    "shutil.copy(f\"{output_zip}.zip\", DRIVE_OUTPUT_PATH)\n",
    "\n",
    "print(f\"‚úÖ Resultados copiados a: {DRIVE_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 9Ô∏è‚É£ Utilidades\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Monitorear GPU en Tiempo Real\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitorear GPU cada 2 segundos (CTRL+C para detener)\n",
    "!watch -n 2 nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Limpiar Memoria GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar memoria GPU\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"‚úÖ Memoria GPU limpiada\")\n",
    "print(f\"Memoria GPU usada: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Memoria GPU reservada: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Ver Procesos en GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver qu√© procesos est√°n usando la GPU\n",
    "!nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ‚úÖ Notebook Completado\n",
    "\n",
    "**Autor:** Rafael Ovalle - Tesis UNAB  \n",
    "**Versi√≥n:** Final  \n",
    "**Fecha:** Noviembre 2025  \n",
    "\n",
    "---\n",
    "\n",
    "## üìå Notas Finales\n",
    "\n",
    "- Este notebook est√° optimizado para **Google Colab + VM local**\n",
    "- Usa `!python3` para todos los comandos\n",
    "- Los datasets V2 ya tienen `train = train + val`\n",
    "- Modelos V1 y V2 est√°n separados en directorios distintos\n",
    "- Recuerda ajustar las rutas de Google Drive seg√∫n tu estructura\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Pr√≥ximos Pasos\n",
    "\n",
    "1. Entrenar con configuraciones √≥ptimas\n",
    "2. Evaluar en test set\n",
    "3. Analizar matriz de confusi√≥n y errores\n",
    "4. Iterar con ajustes de hiperpar√°metros\n",
    "5. Documentar resultados para la tesis\n",
    "\n",
    "---\n",
    "\n",
    "**¬°Buena suerte con el entrenamiento! ü§ü**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
