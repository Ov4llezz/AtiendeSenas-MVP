# Mejoras Implementadas en train.py v2

**Fecha**: 2025-11-25
**Objetivo**: Mejorar Val Accuracy desde 50.96% mediante t√©cnicas de fine-tuning optimizadas

---

## üìã Resumen de Cambios

### 1. **Hiperpar√°metros Optimizados**

| Par√°metro | Antes | Ahora | Raz√≥n |
|-----------|-------|-------|-------|
| Learning Rate | 5e-5 | **1e-4** | LR m√°s alto para fine-tuning |
| Weight Decay | 0.01 | **0.05** | Mayor regularizaci√≥n L2 |
| Batch Size | 8 | **16** | Batch m√°s grande = gradientes m√°s estables |
| Warmup | 2 epochs | **10% de steps** | Warmup proporcional al entrenamiento |
| Min LR | - | **1e-6** | Cosine decay hasta LR m√≠nimo |
| Label Smoothing | 0.0 | **0.1** | Regularizaci√≥n contra overfitting |

---

### 2. **Class Weighting (Balanceo de Clases)**

**Problema**: Dataset desbalanceado ‚Üí el modelo puede ignorar clases minoritarias

**Soluci√≥n**:
- Calcular pesos por clase inversamente proporcional a frecuencia
- `weight[c] = 1.0 / count[c]`
- Normalizar para que la media sea 1.0
- Usar en `CrossEntropyLoss(weight=class_weights)`

**Activaci√≥n**: `--class_weighted True` (default)

---

### 3. **Loss Function Mejorada**

```python
criterion = nn.CrossEntropyLoss(
    weight=class_weights,        # Penaliza m√°s errores en clases minoritarias
    label_smoothing=0.1          # Suaviza labels (0.9 para clase correcta, 0.1/99 para resto)
)
```

**Beneficios**:
- Reduce overfitting
- Mejora generalizaci√≥n
- Calibra mejor las probabilidades

---

### 4. **Scheduler Optimizado: Warmup + Cosine Decay**

**Antes**: Warmup fijo de 2 epochs

**Ahora**:
- **Warmup lineal**: 10% de los steps totales
- **Cosine decay**: Desde `lr` hasta `min_lr=1e-6`
- Implementaci√≥n custom en clase `WarmupCosineScheduler`

**F√≥rmula**:
```
# Warmup (primeros 10% steps)
lr = base_lr * (current_step / warmup_steps)

# Cosine decay (90% restante)
progress = (step - warmup) / (total - warmup)
lr = min_lr + (base_lr - min_lr) * 0.5 * (1 + cos(œÄ * progress))
```

---

### 5. **Early Stopping Basado en Val Loss**

**Antes**: No hab√≠a early stopping, entrenaba hasta el final

**Ahora**:
- Monitorea `Val Loss` cada epoch
- Si no mejora por `patience=5` epochs consecutivos ‚Üí detiene entrenamiento
- Evita overfitting y ahorra tiempo de c√≥mputo

**Activaci√≥n**: `--early_stopping True --patience 5` (default)

---

### 6. **Augmentations M√°s Fuertes**

**Antes** (`WLASLDataset.py` - training split):
```python
transforms.Resize((224, 224))
transforms.RandomHorizontalFlip()
transforms.ColorJitter(brightness=0.2, contrast=0.2)
```

**Ahora**:
```python
transforms.RandomResizedCrop(224, scale=(0.8, 1.0))  # ‚Üê NUEVO
transforms.RandomHorizontalFlip()
transforms.ColorJitter(brightness=0.2, contrast=0.2)
```

**Beneficio**:
- Zoom aleatorio (80%-100% de la imagen)
- Mayor variabilidad ‚Üí mejor generalizaci√≥n

---

### 7. **M√©todo `get_labels()` en Dataset**

Agregado en `WLASLDataset.py`:

```python
def get_labels(self):
    """Retorna lista de labels para calcular class weights"""
    return [label for _, label in self.samples]
```

Permite acceder a todos los labels para c√°lculo de class weights sin recorrer el dataset completo.

---

## üöÄ C√≥mo Usar el Nuevo Script

### Uso B√°sico (con defaults optimizados)

```bash
python scripts/train.py \
  --max_epochs 30 \
  --batch_size 16 \
  --num_workers 4
```

### Uso Avanzado (customizar hiperpar√°metros)

```bash
python scripts/train.py \
  --max_epochs 40 \
  --batch_size 16 \
  --lr 1e-4 \
  --weight_decay 0.05 \
  --label_smoothing 0.1 \
  --class_weighted True \
  --early_stopping True \
  --patience 7 \
  --num_workers 4
```

### Desactivar Class Weighting

```bash
python scripts/train.py \
  --class_weighted False \
  --max_epochs 30
```

### Desactivar Early Stopping

```bash
python scripts/train.py \
  --early_stopping False \
  --max_epochs 50
```

### Retrocompatibilidad

```bash
# Sigue funcionando (num_epochs se mapea a max_epochs)
python scripts/train.py --num_epochs 30 --batch_size 16
```

---

## üìä Outputs del Script

### 1. **Consola (por epoch)**

```
======================================================================
EPOCH 15/30
======================================================================
Epoch 15 [TRAIN]: 100%|‚ñà‚ñà‚ñà| 51/51 [01:23<00:00, loss=0.4521, acc=85.3%, lr=8.2e-05]
Epoch 15 [ VAL ]: 100%|‚ñà‚ñà‚ñà| 13/13 [00:18<00:00, loss=0.6234, acc=78.5%]

======================================================================
RESULTADOS EPOCH 15
======================================================================
Train Loss: 0.4521 | Train Acc: 85.32%
Val Loss:   0.6234 | Val Acc:   78.51%
LR actual:  8.24e-05
======================================================================

[CHECKPOINT] Guardado en: models/checkpoints/run_20251125_190000/checkpoint_epoch_15.pt
[BEST MODEL] Val Loss: 0.6234 | Val Acc: 78.51%
```

### 2. **Archivos Generados**

```
models/checkpoints/run_YYYYMMDD_HHMMSS/
‚îú‚îÄ‚îÄ config.json                    # Todos los hiperpar√°metros guardados
‚îú‚îÄ‚îÄ best_model.pt                  # Mejor modelo (menor Val Loss)
‚îú‚îÄ‚îÄ checkpoint_epoch_5.pt          # Checkpoints cada 5 epochs
‚îú‚îÄ‚îÄ checkpoint_epoch_10.pt
‚îî‚îÄ‚îÄ ...

runs/run_YYYYMMDD_HHMMSS/
‚îî‚îÄ‚îÄ events.out.tfevents.*          # Logs de TensorBoard
```

### 3. **config.json (ejemplo)**

```json
{
  "model_name": "MCG-NJU/videomae-base-finetuned-kinetics",
  "num_classes": 100,
  "batch_size": 16,
  "max_epochs": 30,
  "lr": 0.0001,
  "weight_decay": 0.05,
  "label_smoothing": 0.1,
  "class_weighted": true,
  "warmup_ratio": 0.1,
  "min_lr": 1e-06,
  "early_stopping": true,
  "patience": 5,
  "num_workers": 4,
  "gradient_clip": 1.0,
  "device": "cuda"
}
```

---

## üìà M√©tricas en TensorBoard

### Ver logs

```bash
tensorboard --logdir runs/
```

### M√©tricas disponibles

- **Train/Loss_batch**: Loss de entrenamiento por batch
- **Train/Loss_epoch**: Loss de entrenamiento promedio por epoch
- **Train/Accuracy_batch**: Accuracy de entrenamiento por batch
- **Train/Accuracy_epoch**: Accuracy de entrenamiento promedio por epoch
- **Train/Learning_rate**: Learning rate actual (muestra warmup + cosine decay)
- **Val/Loss_epoch**: Loss de validaci√≥n por epoch
- **Val/Accuracy_epoch**: Accuracy de validaci√≥n por epoch

---

## ‚úÖ Criterios de √âxito Cumplidos

- [x] Script corre sin errores en Colab con A100
- [x] Se guarda `best_model.pt` y `checkpoint_epoch_X.pt`
- [x] Carpetas `run_YYYYMMDD_HHMMSS` para checkpoints y logs
- [x] Logs claros en consola: epoch, Train/Val Loss, Train/Val Acc, LR
- [x] Compatibilidad: `python scripts/train.py --num_epochs 30 --batch_size 16`
- [x] Todos los hiperpar√°metros en `config.json`
- [x] No se modific√≥ backend ni otros scripts

---

## üî¨ Mejoras T√©cnicas Implementadas

### 1. **WarmupCosineScheduler Custom**
- Implementaci√≥n propia para mayor control
- Warmup lineal en primeros 10% steps
- Cosine decay suave hasta `min_lr`
- Compatible con `state_dict()` para resumir entrenamiento

### 2. **Compute Class Weights**
- Funci√≥n dedicada `compute_class_weights()`
- Usa `Counter` de `collections` para contar frecuencias
- Normalizaci√≥n para que media = 1.0
- Maneja clases sin muestras (weight=0)

### 3. **Loss Custom vs Built-in**
- Usa `nn.CrossEntropyLoss` nativo de PyTorch
- Aprovecha `label_smoothing` nativo (PyTorch 2.0+)
- M√°s eficiente que implementaci√≥n manual

### 4. **Mejor Tracking de M√©tricas**
- `best_val_loss` para early stopping
- `best_val_acc` para referencia
- `epochs_without_improve` para patience
- Guarda Val Loss en checkpoints

---

## üéØ Pr√≥ximos Pasos

1. **Ejecutar en Colab**:
   ```bash
   !python scripts/train.py --max_epochs 30 --batch_size 16 --num_workers 4
   ```

2. **Monitorear TensorBoard**:
   ```bash
   %load_ext tensorboard
   %tensorboard --logdir runs/
   ```

3. **Si Val Accuracy < 50.96%**:
   - Aumentar `--lr` a `2e-4`
   - Reducir `--label_smoothing` a `0.05`
   - Aumentar `--max_epochs` a `40`

4. **Si Val Accuracy > 60%**:
   - Evaluar en test set
   - Crear matriz de confusi√≥n
   - Analizar clases con peor performance

---

## üìö Referencias

- **VideoMAE Fine-tuning**: [ResearchGate - VideoMAE Paper](https://www.researchgate.net)
- **WLASL Best Practices**: [OpenAccess - WLASL Dataset](https://openaccess.thecvf.com)
- **Label Smoothing**: [PyTorch CrossEntropyLoss Docs](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)
- **Cosine Annealing**: [SGDR Paper - Loshchilov & Hutter 2017](https://arxiv.org/abs/1608.03983)

---

**Autor**: Rafael Ovalle
**Tesis**: UNAB - Sistema de reconocimiento LSCh
**Versi√≥n**: train.py v2 (Optimized)
