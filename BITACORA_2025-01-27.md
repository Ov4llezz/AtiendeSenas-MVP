# üìã Bit√°cora de Trabajo - AtiendeSenas MVP
**Fecha:** 27 de enero de 2025
**Proyecto:** Sistema de Reconocimiento de Lenguaje de Se√±as - Tesis UNAB
**Autor:** Rafael Ovalle

---

## üéØ Objetivos del D√≠a

1. Crear dataset WLASL300 desde el dataset completo de WLASL
2. Actualizar pipeline de entrenamiento para soportar m√∫ltiples tama√±os de dataset
3. Generar documentaci√≥n completa para entrenamiento
4. Verificar integridad de los datasets
5. Definir hiperpar√°metros √≥ptimos para entrenamiento

---

## ‚úÖ Tareas Completadas

### 1. Creaci√≥n de WLASL300 Dataset

**Archivo creado:** `scripts/create_wlasl300.py`

**Descripci√≥n:**
- Script para generar WLASL300 desde el dataset completo en `C:\Users\ov4ll\Desktop\TESIS\WLASL_full`
- Selecciona las top 300 glosas con m√°s videos
- Mantiene la misma estructura que WLASL100

**Resultado:**
```
‚úì Dataset WLASL300 creado exitosamente
‚úì 300 clases seleccionadas
‚úì 2,790 videos totales
  - Train: 1,960 videos (70.3%)
  - Val: 558 videos (20.0%)
  - Test: 272 videos (9.7%)
‚úì Archivos generados:
  - data/wlasl300/nslt_300.json
  - data/wlasl300/WLASL_v0.3_300.json
  - data/wlasl300/gloss_to_id.json
  - data/wlasl300/splits/ (train/val/test)
```

**Commits:**
- `Create WLASL300 dataset generation script`
- `Fix Unicode encoding for Windows compatibility`

---

### 2. Actualizaci√≥n de Scripts Core

#### 2.1 WLASLDataset.py

**Modificaciones:**
- A√±adido par√°metro `dataset_size` (100 o 300)
- Auto-detecci√≥n de archivos JSON seg√∫n tama√±o
- Soporte para m√∫ltiples configuraciones de dataset

**C√≥digo clave:**
```python
def __init__(
    self,
    split: str,
    dataset_size: int = 100,  # NUEVO
    ...
):
    if self.dataset_size == 300:
        if meta_json == "WLASL_v0.3.json":
            meta_json = "WLASL_v0.3_300.json"
        if subset_json == "nslt_100.json":
            subset_json = "nslt_300.json"
```

#### 2.2 train.py

**Modificaciones:**
- A√±adido par√°metro `--dataset` para seleccionar wlasl100 o wlasl300
- Auto-configuraci√≥n de rutas y n√∫mero de clases
- Compatibilidad retroactiva con configuraciones antiguas

**Uso:**
```bash
python scripts/train.py --dataset wlasl100  # 100 clases
python scripts/train.py --dataset wlasl300  # 300 clases
```

#### 2.3 test.py

**Modificaciones:**
- Auto-detecci√≥n de dataset desde configuraci√≥n de entrenamiento
- Carga autom√°tica del checkpoint correcto
- Soporte para ambos tama√±os de dataset

**Commit:**
- `Update training pipeline to support WLASL100 and WLASL300`

---

### 3. Documentaci√≥n Completa

#### 3.1 DATASETS_README.md

**Contenido:**
- Descripci√≥n de ambos datasets (WLASL100 y WLASL300)
- Estructura de directorios
- Comandos de entrenamiento y evaluaci√≥n
- Configuraci√≥n de hiperpar√°metros
- Ejemplos de uso
- Troubleshooting

#### 3.2 COLAB_QUICKSTART.md

**Contenido:**
- Gu√≠a r√°pida para Google Colab
- Setup paso a paso
- Configuraci√≥n de GPU
- Integraci√≥n con Google Drive
- Comandos comunes

#### 3.3 AtiendeSenas_Training_Colab.ipynb

**Caracter√≠sticas:**
- Notebook completo para entrenamiento en Google Colab
- Verificaci√≥n de GPU
- Instalaci√≥n autom√°tica de dependencias
- Configuraci√≥n interactiva de hiperpar√°metros
- TensorBoard integrado
- Evaluaci√≥n y visualizaci√≥n de resultados
- Descarga de checkpoints

**Commit:**
- `Add comprehensive documentation and Google Colab notebook`

---

### 4. Reportes de Datasets

#### 4.1 WLASL300_DATASET_REPORT.txt

**Archivo creado:** `scripts/generate_wlasl300_report.py`

**Contenido del reporte:**
- Resumen general: 2,790 videos, 300 glosas
- Estad√≠sticas de distribuci√≥n:
  - Promedio: 9.4 videos/glosa
  - M√≠nimo: 1 video/glosa
  - M√°ximo: 16 videos/glosa
  - Desbalance: Alto (ratio 16:1)
- Tabla completa de las 300 glosas con splits
- Top 20 glosas con m√°s videos
- Top 20 glosas con menos videos

#### 4.2 WLASL100_DATASET_REPORT.txt (CORREGIDO)

**Archivo creado:** `scripts/generate_wlasl100_report.py`

**Problema detectado:**
- Versi√≥n inicial contaba todos los videos en `nslt_100.json` (2,038 videos)
- Solo 1,118 videos est√°n descargados localmente

**Soluci√≥n aplicada:**
- Script corregido para leer archivos de splits
- Cuenta solo videos que existen f√≠sicamente

**Estad√≠sticas corregidas:**
```
ANTES (INCORRECTO):
- Total: 2,038 videos
- Train: 1,442 videos
- Val: 338 videos
- Test: 258 videos

DESPU√âS (CORRECTO):
- Total: 1,118 videos
- Train: 807 videos (72.2%)
- Val: 194 videos (17.4%)
- Test: 117 videos (10.5%)
- Promedio: 11.2 videos/glosa
- M√≠nimo: 6 videos/glosa
- M√°ximo: 19 videos/glosa
- Desbalance: Moderado (ratio 3.2:1)
```

**Commits:**
- `Add WLASL300 dataset report generator`
- `Add WLASL100 dataset report (CORRECTED version)`

---

### 5. Gu√≠a de Hiperpar√°metros

**Archivo creado:** `HYPERPARAMETERS_GUIDE.md`

**Contenido:**

#### 5.1 An√°lisis de Datasets
- WLASL100: 1,118 videos, 100 clases, desbalance moderado
- WLASL300: 2,790 videos, 300 clases, desbalance alto

#### 5.2 Configuraciones Recomendadas

**Para WLASL100 (Baseline):**
```python
DATASET = "wlasl100"
BATCH_SIZE = 16
MAX_EPOCHS = 30
LEARNING_RATE = 1e-4
WEIGHT_DECAY = 0.05
LABEL_SMOOTHING = 0.1
CLASS_WEIGHTED = True
WARMUP_RATIO = 0.1
PATIENCE = 5
```
- **Esperado:** 40-55% validation accuracy
- **Tiempo:** 2-4 horas
- **Uso:** Baseline para empezar

**Para WLASL100 (Agresivo):**
```python
BATCH_SIZE = 12
MAX_EPOCHS = 50
LEARNING_RATE = 5e-5
WEIGHT_DECAY = 0.1
LABEL_SMOOTHING = 0.15
PATIENCE = 7
```
- **Esperado:** 45-60% validation accuracy
- **Tiempo:** 4-6 horas
- **Uso:** Maximizar accuracy

**Para WLASL300 (Recomendado):**
```python
DATASET = "wlasl300"
BATCH_SIZE = 12
MAX_EPOCHS = 50
LEARNING_RATE = 5e-5
WEIGHT_DECAY = 0.08
LABEL_SMOOTHING = 0.15
CLASS_WEIGHTED = True
WARMUP_RATIO = 0.15
PATIENCE = 7
```
- **Esperado:** 25-40% validation accuracy, 50-65% top-5
- **Tiempo:** 8-12 horas
- **Uso:** Demostrar escalabilidad

#### 5.3 Justificaci√≥n Cient√≠fica

Cada recomendaci√≥n incluye:
- **Learning Rate (1e-4 a 5e-5):** Transfer learning de VideoMAE pre-entrenado
- **Batch Size (12-16):** Datasets peque√±os requieren batches peque√±os
- **Weight Decay (0.05-0.1):** Est√°ndar para Vision Transformers
- **Label Smoothing (0.1-0.15):** Previene overconfidence con pocos datos
- **Class Weighting:** CR√çTICO para desbalance (3.2:1 y 16:1)
- **Warmup (10-15%):** Evita gradientes explosivos al inicio

#### 5.4 Referencias Citadas
1. Tong et al. (2022): "VideoMAE: Masked Autoencoders are Data-Efficient Learners"
2. Dosovitskiy et al. (2021): "An Image is Worth 16x16 Words: Transformers for Image Recognition"
3. Loshchilov & Hutter (2019): "Decoupled Weight Decay Regularization"
4. Szegedy et al. (2016): "Rethinking the Inception Architecture"
5. Cui et al. (2019): "Class-Balanced Loss Based on Effective Number of Samples"
6. Masters & Luschi (2018): "Revisiting Small Batch Training for Deep Neural Networks"

#### 5.5 Estrategia de Experimentaci√≥n

**Fase 1: Baseline (WLASL100)**
```bash
python scripts/train.py --dataset wlasl100 --batch_size 16 --max_epochs 30 --lr 1e-4
```

**Fase 2: Fine-tuning (WLASL100)**
```bash
python scripts/train.py --dataset wlasl100 --batch_size 12 --max_epochs 50 --lr 5e-5 --weight_decay 0.08
```

**Fase 3: Escalabilidad (WLASL300)**
```bash
python scripts/train.py --dataset wlasl300 --batch_size 12 --max_epochs 50 --lr 5e-5 --weight_decay 0.08 --label_smoothing 0.15
```

#### 5.6 Grid Search Sugerido

| Par√°metro | WLASL100 | WLASL300 | Prioridad |
|-----------|----------|----------|-----------|
| Learning Rate | [5e-5, 1e-4, 2e-4] | [3e-5, 5e-5, 8e-5] | ‚≠ê‚≠ê‚≠ê Alta |
| Batch Size | [12, 16] | [8, 12, 16] | ‚≠ê‚≠ê Media |
| Weight Decay | [0.05, 0.08, 0.1] | [0.06, 0.08, 0.1] | ‚≠ê‚≠ê Media |
| Label Smoothing | [0.1, 0.15] | [0.12, 0.15] | ‚≠ê Baja |

#### 5.7 Se√±ales de Alerta

**Overfitting:**
- S√≠ntomas: Train Acc > 80%, Val Acc < 50%
- Soluci√≥n: ‚Üë Weight decay, ‚Üë Label smoothing, ‚Üì Batch size

**Underfitting:**
- S√≠ntomas: Train Acc < 60%, Val Acc ‚âà Train Acc
- Soluci√≥n: ‚Üë Learning rate, ‚Üë Max epochs, ‚Üì Weight decay

**Inestabilidad:**
- S√≠ntomas: Loss oscila mucho, no converge
- Soluci√≥n: ‚Üì Learning rate, ‚Üë Warmup ratio, ‚Üì Gradient clip

**Commit:**
- `Add comprehensive hyperparameters guide for VideoMAE training`

---

### 6. Verificaci√≥n de Integridad de Videos

**Archivo creado:** `scripts/check_corrupted_videos.py`

**Funcionalidad:**
- Verifica todos los videos en WLASL300 (train, val, test)
- Usa OpenCV para verificar que cada video puede:
  - Abrirse correctamente
  - Leer al menos un frame
  - Tiene FPS v√°lido
  - Tiene resoluci√≥n v√°lida
  - Tiene frame count > 0

**Resultados de verificaci√≥n:**
```
================================================================================
RESUMEN FINAL - WLASL300 COMPLETO
================================================================================
Total videos verificados: 2,790
Videos corruptos totales: 0
Train corruptos:          0
Val corruptos:            0
Test corruptos:           0
================================================================================

DETALLES:
- Train: 1,960 videos ‚Üí 1,960 v√°lidos (100.0%)
- Val: 558 videos ‚Üí 558 v√°lidos (100.0%)
- Test: 272 videos ‚Üí 272 v√°lidos (100.0%)
```

**Conclusi√≥n:** ‚úÖ **Todos los videos est√°n en perfecto estado para entrenamiento**

---

## üìä Resumen de Archivos Creados/Modificados

### Scripts Nuevos
1. `scripts/create_wlasl300.py` - Generador de dataset WLASL300
2. `scripts/generate_wlasl300_report.py` - Reporte de WLASL300
3. `scripts/generate_wlasl100_report.py` - Reporte de WLASL100 (corregido)
4. `scripts/check_corrupted_videos.py` - Verificador de integridad de videos
5. `scripts/verify_datasets.py` - Verificador de estructura de datasets

### Scripts Modificados
1. `scripts/WLASLDataset.py` - Soporte para dataset_size variable
2. `scripts/train.py` - Par√°metro --dataset y auto-configuraci√≥n
3. `scripts/test.py` - Auto-detecci√≥n de dataset

### Documentaci√≥n
1. `DATASETS_README.md` - Gu√≠a completa de uso de datasets
2. `COLAB_QUICKSTART.md` - Quick start para Google Colab
3. `HYPERPARAMETERS_GUIDE.md` - Gu√≠a cient√≠fica de hiperpar√°metros
4. `AtiendeSenas_Training_Colab.ipynb` - Notebook completo para Colab

### Reportes Generados
1. `WLASL100_DATASET_REPORT.txt` - Estad√≠sticas de WLASL100
2. `WLASL300_DATASET_REPORT.txt` - Estad√≠sticas de WLASL300

### Dataset Generado
1. `data/wlasl300/` - Dataset completo con 2,790 videos
   - `nslt_300.json`
   - `WLASL_v0.3_300.json`
   - `gloss_to_id.json`
   - `splits/train_split.txt`
   - `splits/val_split.txt`
   - `splits/test_split.txt`

---

## üîß Problemas Resueltos

### Problema 1: Encoding Unicode en Windows
**Error:** `UnicodeEncodeError: 'charmap' codec can't encode character '\u2713'`

**Causa:** Uso de caracteres Unicode (‚úì, ‚ùå) en prints con codificaci√≥n CP1252 de Windows

**Soluci√≥n:** Reemplazar todos los caracteres Unicode por ASCII:
- `‚úì` ‚Üí `[OK]`
- `‚ùå` ‚Üí `[X]`

**Archivos afectados:**
- `create_wlasl300.py`
- `verify_datasets.py`
- `generate_wlasl300_report.py`
- `generate_wlasl100_report.py`

### Problema 2: Reporte WLASL100 Incorrecto
**Error:** Reportaba 1,442 videos en train cuando solo hay 807

**Causa:** Script contaba todos los videos en `nslt_100.json` (dataset completo) en lugar de solo los videos descargados localmente

**Soluci√≥n:** Modificar script para leer archivos de splits y contar solo videos existentes

**Impacto:** Correcci√≥n cr√≠tica que afecta estad√≠sticas reportadas en tesis

### Problema 3: Git Push Rejection
**Error:** Remote contains work that local doesn't have

**Soluci√≥n:**
```bash
git stash && git pull --rebase && git push && git stash pop
```

---

## üìà M√©tricas y Estad√≠sticas

### WLASL100 (Dataset Local)
- **Videos totales:** 1,118
- **Clases:** 100
- **Distribuci√≥n:**
  - Train: 807 (72.2%)
  - Val: 194 (17.4%)
  - Test: 117 (10.5%)
- **Videos/clase:**
  - Promedio: 11.2
  - M√≠nimo: 6 (tabla)
  - M√°ximo: 19 (computer)
- **Desbalance:** Moderado (ratio 3.2:1)

### WLASL300 (Generado Hoy)
- **Videos totales:** 2,790
- **Clases:** 300
- **Distribuci√≥n:**
  - Train: 1,960 (70.3%)
  - Val: 558 (20.0%)
  - Test: 272 (9.7%)
- **Videos/clase:**
  - Promedio: 9.4
  - M√≠nimo: 1
  - M√°ximo: 16
- **Desbalance:** Alto (ratio 16:1)

---

## üöÄ Estado del Proyecto

### ‚úÖ Completado
- [x] Dataset WLASL300 creado y verificado
- [x] Pipeline de entrenamiento actualizado para ambos datasets
- [x] Documentaci√≥n completa para entrenamiento local y Colab
- [x] Reportes estad√≠sticos detallados
- [x] Gu√≠a de hiperpar√°metros con justificaci√≥n cient√≠fica
- [x] Verificaci√≥n de integridad de videos (0 corruptos)
- [x] Todo el c√≥digo subido a GitHub

### üìã Siguiente Fase (Pendiente)
- [ ] Entrenamiento baseline con WLASL100
- [ ] Fine-tuning de hiperpar√°metros
- [ ] Entrenamiento con WLASL300
- [ ] An√°lisis de resultados
- [ ] Generaci√≥n de m√©tricas para tesis
- [ ] Comparaci√≥n WLASL100 vs WLASL300

---

## üíæ Commits Realizados

1. `Create WLASL300 dataset generation script`
2. `Fix Unicode encoding for Windows compatibility`
3. `Update training pipeline to support WLASL100 and WLASL300`
4. `Add dataset verification script`
5. `Add comprehensive documentation and Google Colab notebook`
6. `Add WLASL300 dataset report generator`
7. `Generate WLASL300 dataset report`
8. `Add WLASL100 dataset report generator`
9. `Fix WLASL100 report to count only local videos`
10. `Add comprehensive hyperparameters guide for VideoMAE training`

**Repositorio:** https://github.com/Ov4llezz/AtiendeSenas-MVP

---

## üìö Referencias Utilizadas

1. **VideoMAE Paper** (Tong et al., 2022)
   - Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training

2. **Vision Transformer** (Dosovitskiy et al., 2021)
   - An Image is Worth 16x16 Words: Transformers for Image Recognition

3. **AdamW** (Loshchilov & Hutter, 2019)
   - Decoupled Weight Decay Regularization

4. **Label Smoothing** (Szegedy et al., 2016)
   - Rethinking the Inception Architecture for Computer Vision

5. **Class Balancing** (Cui et al., 2019)
   - Class-Balanced Loss Based on Effective Number of Samples

6. **Small Batch Training** (Masters & Luschi, 2018)
   - Revisiting Small Batch Training for Deep Neural Networks

---

## üéì Notas para la Tesis

### Contribuciones Realizadas

1. **Escalabilidad del modelo:**
   - Demostraci√≥n de que el pipeline funciona con 100 y 300 clases
   - Permite validar generalizaci√≥n del modelo

2. **An√°lisis de datasets:**
   - Estad√≠sticas detalladas de distribuci√≥n de clases
   - Identificaci√≥n de desbalance (cr√≠tico para m√©tricas)

3. **Configuraci√≥n cient√≠ficamente respaldada:**
   - Hiperpar√°metros basados en literatura
   - Justificaci√≥n te√≥rica para cada elecci√≥n
   - Referencias a papers relevantes

4. **Reproducibilidad:**
   - C√≥digo documentado
   - Scripts automatizados
   - Notebook de Colab para facilitar experimentos

### M√©tricas Sugeridas para Reportar

1. **Accuracy:**
   - Top-1 accuracy (total, por clase, por split)
   - Top-5 accuracy (especialmente para WLASL300)

2. **Precision, Recall, F1-Score:**
   - Macro-averaged (todas las clases igual peso)
   - Weighted-averaged (ponderado por soporte)

3. **Matrices de Confusi√≥n:**
   - Identificar clases frecuentemente confundidas
   - An√°lisis de errores sistem√°ticos

4. **Curvas de Aprendizaje:**
   - Train/Val loss por epoch
   - Train/Val accuracy por epoch
   - Evidencia de overfitting/underfitting

5. **Comparaci√≥n WLASL100 vs WLASL300:**
   - Accuracy relativo
   - Tiempo de convergencia
   - Generalizaci√≥n del modelo

---

## ‚è±Ô∏è Tiempo Estimado de Trabajo

**Hoy:** ~3-4 horas de desarrollo y documentaci√≥n

**Pr√≥ximas fases:**
- Entrenamiento WLASL100 baseline: 2-4 horas
- Experimentaci√≥n con hiperpar√°metros: 8-12 horas
- Entrenamiento WLASL300: 8-12 horas
- An√°lisis de resultados: 4-6 horas
- **Total estimado:** 22-34 horas de GPU time

---

## üîÑ Siguientes Pasos Recomendados

1. **Inmediato (esta semana):**
   - Entrenar baseline WLASL100 con configuraci√≥n recomendada
   - Validar que el pipeline funciona end-to-end
   - Generar primeras m√©tricas

2. **Corto plazo (pr√≥ximas 2 semanas):**
   - Experimentar con grid search de hiperpar√°metros
   - Entrenar WLASL300 para demostrar escalabilidad
   - Documentar resultados

3. **Mediano plazo (siguiente mes):**
   - An√°lisis profundo de resultados
   - Generaci√≥n de visualizaciones para tesis
   - Comparaci√≥n con state-of-the-art

---

**Fin de bit√°cora - 27 de enero de 2025**

---

## üìù Notas Finales

Todo el c√≥digo est√° subido a GitHub y listo para usar. Los datasets est√°n verificados y libres de archivos corruptos. La documentaci√≥n est√° completa tanto para entrenamiento local como en Google Colab.

El proyecto est√° en excelente estado para comenzar la fase de experimentaci√≥n.

**¬°√âxito con el entrenamiento y la tesis!** üöÄ
