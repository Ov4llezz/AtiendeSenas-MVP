{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ñ VideoMAE - Reconocimiento de Lengua de Se√±as (WLASL)\n",
        "\n",
        "**Proyecto**: Reconocimiento de Lengua de Se√±as Americana con VideoMAE\n",
        "\n",
        "**Autor**: Rafael Ovalle - Tesis UNAB\n",
        "\n",
        "**Modelo**: VideoMAE (Video Masked Autoencoder) de Microsoft Research\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Configuraciones Disponibles\n",
        "\n",
        "| Config | Dataset | Train | Val | Test | Batch | LR | Regularizaci√≥n | Uso |\n",
        "|--------|---------|-------|-----|------|-------|----|--------------|---------|\n",
        "| **V1-100** | 100 clases | 807 | 194 | 117 | 16 | 1e-4 | Alta | Baseline |\n",
        "| **V2-100** | 100 clases | 1,001 | 117 | 117 | 6 | 1e-5 | Baja | Maximizar datos |\n",
        "| **V1-300** | 300 clases | 1,959 | 557 | 271 | 16 | 1e-4 | Alta | Baseline |\n",
        "| **V2-300** | 300 clases | 2,516 | 271 | 271 | 6 | 1e-5 | Baja | Maximizar datos |\n",
        "\n",
        "**Diferencias V1 vs V2:**\n",
        "- **V1**: Split tradicional (train/val/test separados) + regularizaci√≥n completa\n",
        "- **V2**: Train+Val combinados, Test como validaci√≥n + regularizaci√≥n reducida\n",
        "\n",
        "---\n",
        "\n",
        "## üìö √çndice\n",
        "1. [Configuraci√≥n Inicial](#1)\n",
        "2. [Verificar y Copiar Datasets](#2)\n",
        "3. [Configurar Experimento](#3)\n",
        "4. [Cargar Datos](#4)\n",
        "5. [Entrenamiento](#5)\n",
        "6. [Evaluaci√≥n](#6)\n",
        "7. [Visualizaciones](#7)\n",
        "8. [Guardar y Descargar Resultados](#8)\n",
        "9. [Utilidades](#9)\n",
        "10. [Soluci√≥n de Problemas](#10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1Ô∏è‚É£ Configuraci√≥n Inicial <a id=\"1\"></a>\n",
        "\n",
        "### Verificar GPU y Entorno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar informaci√≥n del sistema\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"INFORMACI√ìN DEL SISTEMA\".center(70))\n",
        "print(\"=\"*70)\n",
        "print(f\"Python:  {sys.version.split()[0]}\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA:    {torch.version.cuda if torch.cuda.is_available() else 'No disponible'}\")\n",
        "print(f\"\\nGPU disponible: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "    print(f\"Memoria GPU: {gpu_memory_gb:.2f} GB\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è WARNING: GPU no disponible!\")\n",
        "    print(\"   El entrenamiento ser√° MUY lento.\")\n",
        "    print(\"   Soluci√≥n: Runtime > Change runtime type > GPU\")\n",
        "\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar GPU con nvidia-smi\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Montar Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Ruta donde guardas tus datasets y resultados en Drive\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive/TESIS_WLASL\"\n",
        "print(f\"‚úÖ Drive montado: {DRIVE_ROOT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clonar Repositorio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Clonar repositorio si no existe\n",
        "if not os.path.exists('AtiendeSenas-MVP'):\n",
        "    print(\"[INFO] Clonando repositorio...\")\n",
        "    !git clone https://github.com/Ov4llezz/AtiendeSenas-MVP.git\n",
        "    %cd AtiendeSenas-MVP\n",
        "else:\n",
        "    print(\"[INFO] Repositorio ya existe, actualizando...\")\n",
        "    %cd AtiendeSenas-MVP\n",
        "    !git pull\n",
        "\n",
        "print(\"\\n‚úÖ Repositorio listo\")\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instalar Dependencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instalar todas las dependencias necesarias\n",
        "from colab_utils.config import setup_environment\n",
        "\n",
        "print(\"[INFO] Instalando dependencias...\\n\")\n",
        "setup_environment()\n",
        "print(\"\\n‚úÖ Todas las dependencias instaladas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2Ô∏è‚É£ Verificar y Copiar Datasets <a id=\"2\"></a>\n",
        "\n",
        "### Copiar Datasets desde Google Drive (Opcional)\n",
        "\n",
        "Si tienes los datasets en Google Drive, c√≥pialos al entorno de Colab para entrenamiento m√°s r√°pido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# ============================================================\n",
        "# CONFIGURA LAS RUTAS DE TUS DATASETS EN DRIVE\n",
        "# ============================================================\n",
        "DRIVE_DATASETS = \"/content/drive/MyDrive/WLASL\"  # Cambia esto a tu ruta\n",
        "# ============================================================\n",
        "\n",
        "# Rutas origen\n",
        "src_wlasl100 = os.path.join(DRIVE_DATASETS, \"wlasl100\")\n",
        "src_wlasl300 = os.path.join(DRIVE_DATASETS, \"wlasl300\")\n",
        "\n",
        "# Ruta destino\n",
        "dest_base = \"/content/AtiendeSenas-MVP/data\"\n",
        "os.makedirs(dest_base, exist_ok=True)\n",
        "\n",
        "def copiar_dataset(src, dest_base, dataset_name):\n",
        "    \"\"\"Copia dataset desde Drive a Colab\"\"\"\n",
        "    if not os.path.exists(src):\n",
        "        print(f\"‚ùå No se encontr√≥: {src}\")\n",
        "        return False\n",
        "    \n",
        "    destino = os.path.join(dest_base, dataset_name)\n",
        "    \n",
        "    # Si ya existe, preguntar si sobrescribir\n",
        "    if os.path.exists(destino):\n",
        "        print(f\"‚ö†Ô∏è  {dataset_name} ya existe en Colab, saltando...\")\n",
        "        return True\n",
        "    \n",
        "    print(f\"üì¶ Copiando {dataset_name}... (esto puede tomar varios minutos)\")\n",
        "    shutil.copytree(src, destino)\n",
        "    print(f\"‚úÖ {dataset_name} copiado exitosamente\")\n",
        "    return True\n",
        "\n",
        "# Copiar datasets\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"COPIAR DATASETS DESDE DRIVE\".center(70))\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "copiar_dataset(src_wlasl100, dest_base, \"wlasl100\")\n",
        "copiar_dataset(src_wlasl300, dest_base, \"wlasl300\")\n",
        "\n",
        "print(\"\\n‚úÖ Proceso completado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verificar Estructura de Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def verificar_dataset(dataset_path, dataset_name):\n",
        "    \"\"\"Verifica estructura y cuenta videos en cada split\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"DATASET: {dataset_name}\".center(60))\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    if not os.path.exists(dataset_path):\n",
        "        print(f\"‚ùå No encontrado: {dataset_path}\")\n",
        "        return\n",
        "    \n",
        "    splits = ['train', 'val', 'test']\n",
        "    total_videos = 0\n",
        "    \n",
        "    for split in splits:\n",
        "        split_path = os.path.join(dataset_path, 'dataset', split)\n",
        "        \n",
        "        if os.path.exists(split_path):\n",
        "            videos = [f for f in os.listdir(split_path) if f.endswith('.mp4')]\n",
        "            num_videos = len(videos)\n",
        "            total_videos += num_videos\n",
        "            print(f\"{split.capitalize():5s}: {num_videos:5d} videos\")\n",
        "        else:\n",
        "            print(f\"{split.capitalize():5s}: ‚ùå No existe\")\n",
        "    \n",
        "    print(f\"{'‚îÄ'*60}\")\n",
        "    print(f\"TOTAL: {total_videos:5d} videos\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# Verificar ambos datasets\n",
        "verificar_dataset('data/wlasl100', 'WLASL100')\n",
        "verificar_dataset('data/wlasl300', 'WLASL300')\n",
        "\n",
        "# Verificar versiones V2 si existen\n",
        "if os.path.exists('data/wlasl100_v2'):\n",
        "    verificar_dataset('data/wlasl100_v2', 'WLASL100_V2')\n",
        "if os.path.exists('data/wlasl300_v2'):\n",
        "    verificar_dataset('data/wlasl300_v2', 'WLASL300_V2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3Ô∏è‚É£ Configurar Experimento <a id=\"3\"></a>\n",
        "\n",
        "### üéØ Selecciona tu configuraci√≥n de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from colab_utils.config import create_config, print_config, save_config\n",
        "\n",
        "# ============================================================\n",
        "#   üéØ CONFIGURA TU EXPERIMENTO AQU√ç\n",
        "# ============================================================\n",
        "\n",
        "DATASET_TYPE = \"wlasl100\"  # Opciones: \"wlasl100\" o \"wlasl300\"\n",
        "VERSION = \"v1\"             # Opciones: \"v1\" (baseline) o \"v2\" (experimental)\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "# Crear configuraci√≥n autom√°tica\n",
        "config = create_config(\n",
        "    dataset_type=DATASET_TYPE,\n",
        "    version=VERSION,\n",
        "    drive_root=DRIVE_ROOT\n",
        ")\n",
        "\n",
        "# Mostrar configuraci√≥n\n",
        "print_config(config)\n",
        "\n",
        "# Guardar configuraci√≥n (se guardar√° en results/)\n",
        "save_config(config, config['results_dir'])\n",
        "\n",
        "print(f\"\\n‚úÖ Configuraci√≥n creada: {DATASET_TYPE.upper()} {VERSION.upper()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚öôÔ∏è Ajustes Manuales (Opcional)\n",
        "\n",
        "Si quieres personalizar hiperpar√°metros espec√≠ficos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Descomenta y modifica los par√°metros que quieras cambiar\n",
        "\n",
        "# config['batch_size'] = 8          # Cambiar batch size\n",
        "# config['lr'] = 1e-4               # Cambiar learning rate\n",
        "# config['max_epochs'] = 50         # Cambiar n√∫mero de epochs\n",
        "# config['patience'] = 10           # Cambiar paciencia early stopping\n",
        "# config['num_workers'] = 2         # Cambiar workers DataLoader\n",
        "\n",
        "print(\"Configuraci√≥n actual:\")\n",
        "print(f\"  Batch size:  {config['batch_size']}\")\n",
        "print(f\"  Learning rate: {config['lr']}\")\n",
        "print(f\"  Max epochs:  {config['max_epochs']}\")\n",
        "print(f\"  Patience:    {config['patience']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4Ô∏è‚É£ Cargar Datos <a id=\"4\"></a>\n",
        "\n",
        "### Crear Datasets y DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from colab_utils.dataset import WLASLVideoDataset\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CARGANDO DATASETS\".center(70))\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Crear datasets\n",
        "print(\"[1/3] Creando Train Dataset...\")\n",
        "train_dataset = WLASLVideoDataset(\n",
        "    split=\"train\",\n",
        "    base_path=config['data_root'],\n",
        "    dataset_size=config['num_classes']\n",
        ")\n",
        "\n",
        "print(\"[2/3] Creando Validation Dataset...\")\n",
        "val_dataset = WLASLVideoDataset(\n",
        "    split=\"val\",\n",
        "    base_path=config['data_root'],\n",
        "    dataset_size=config['num_classes']\n",
        ")\n",
        "\n",
        "print(\"[3/3] Creando Test Dataset...\")\n",
        "test_dataset = WLASLVideoDataset(\n",
        "    split=\"test\",\n",
        "    base_path=config['data_root'],\n",
        "    dataset_size=config['num_classes']\n",
        ")\n",
        "\n",
        "# Crear dataloaders\n",
        "print(\"\\n[INFO] Creando DataLoaders...\")\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    num_workers=config['num_workers'],\n",
        "    pin_memory=True if config['device'] == \"cuda\" else False\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=False,\n",
        "    num_workers=config['num_workers'],\n",
        "    pin_memory=True if config['device'] == \"cuda\" else False\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=False,\n",
        "    num_workers=config['num_workers'],\n",
        "    pin_memory=True if config['device'] == \"cuda\" else False\n",
        ")\n",
        "\n",
        "# Resumen\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DATASETS CARGADOS EXITOSAMENTE\".center(70))\n",
        "print(\"=\"*70)\n",
        "print(f\"Train:      {len(train_dataset):>5,} videos ({len(train_loader):>4} batches)\")\n",
        "print(f\"Validation: {len(val_dataset):>5,} videos ({len(val_loader):>4} batches)\")\n",
        "print(f\"Test:       {len(test_dataset):>5,} videos ({len(test_loader):>4} batches)\")\n",
        "print(f\"\\nClases:     {config['num_classes']}\")\n",
        "print(f\"Batch size: {config['batch_size']}\")\n",
        "print(\"=\"*70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5Ô∏è‚É£ Entrenamiento <a id=\"5\"></a>\n",
        "\n",
        "### Iniciar Entrenamiento Completo\n",
        "\n",
        "‚è∞ **Tiempo estimado:**\n",
        "- WLASL100 V1: ~2-3 horas (GPU T4)\n",
        "- WLASL100 V2: ~3-4 horas (GPU T4)\n",
        "- WLASL300 V1: ~6-8 horas (GPU T4)\n",
        "- WLASL300 V2: ~8-12 horas (GPU T4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from colab_utils.training import train_model\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üöÄ INICIANDO ENTRENAMIENTO\".center(70))\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Entrenar modelo\n",
        "model, training_history, run_checkpoint_dir, log_dir = train_model(\n",
        "    config=config,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    train_dataset=train_dataset\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ ENTRENAMIENTO COMPLETADO\".center(70))\n",
        "print(\"=\"*70)\n",
        "print(f\"Checkpoints: {run_checkpoint_dir}\")\n",
        "print(f\"Logs:        {log_dir}\")\n",
        "print(\"=\"*70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Entrenamiento R√°pido (Solo para Pruebas)\n",
        "\n",
        "Si solo quieres verificar que todo funciona correctamente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLO DESCOMENTA SI QUIERES PRUEBA R√ÅPIDA (1-2 epochs)\n",
        "\n",
        "# config_test = config.copy()\n",
        "# config_test['max_epochs'] = 2\n",
        "# config_test['patience'] = 5\n",
        "\n",
        "# print(\"‚ö° ENTRENAMIENTO DE PRUEBA (2 epochs)\\n\")\n",
        "# model, training_history, run_checkpoint_dir, log_dir = train_model(\n",
        "#     config=config_test,\n",
        "#     train_loader=train_loader,\n",
        "#     val_loader=val_loader,\n",
        "#     train_dataset=train_dataset\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizar TensorBoard\n",
        "\n",
        "Monitorea el entrenamiento en tiempo real:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar extensi√≥n TensorBoard\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Lanzar TensorBoard\n",
        "%tensorboard --logdir {log_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6Ô∏è‚É£ Evaluaci√≥n <a id=\"6\"></a>\n",
        "\n",
        "### Cargar Mejor Modelo y Evaluar en Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from colab_utils.evaluation import evaluate_detailed, print_results, print_top_classes\n",
        "import torch\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CARGANDO MEJOR MODELO\".center(70))\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Cargar mejor modelo\n",
        "best_model_path = f\"{run_checkpoint_dir}/best_model.pt\"\n",
        "checkpoint = torch.load(best_model_path, map_location=config['device'])\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "print(f\"‚úÖ Modelo cargado exitosamente\")\n",
        "print(f\"   Epoch:    {checkpoint['epoch']}\")\n",
        "print(f\"   Val Loss: {checkpoint['val_loss']:.4f}\")\n",
        "print(f\"   Val Acc:  {checkpoint['val_acc']:.2f}%\")\n",
        "\n",
        "# Evaluar en test set\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EVALUACI√ìN EN TEST SET\".center(70))\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "test_results = evaluate_detailed(\n",
        "    model=model,\n",
        "    dataloader=test_loader,\n",
        "    device=config['device'],\n",
        "    num_classes=config['num_classes']\n",
        ")\n",
        "\n",
        "# Mostrar resultados generales\n",
        "print_results(test_results)\n",
        "\n",
        "# Mostrar mejores y peores clases\n",
        "print_top_classes(test_results, top_n=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7Ô∏è‚É£ Visualizaciones <a id=\"7\"></a>\n",
        "\n",
        "### Curvas de Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from colab_utils.visualization import plot_training_curves\n",
        "from datetime import datetime\n",
        "\n",
        "# Preparar datos\n",
        "history_df = pd.DataFrame(training_history)\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Graficar curvas de entrenamiento\n",
        "curves_path = f\"{config['results_dir']}/training_curves_{timestamp}.png\"\n",
        "plot_training_curves(history_df, save_path=curves_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Todas las Visualizaciones\n",
        "\n",
        "Genera todas las visualizaciones: matriz de confusi√≥n, performance por clase, distribuciones, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from colab_utils.visualization import visualize_all_results\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"GENERANDO VISUALIZACIONES\".center(70))\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Generar todas las visualizaciones\n",
        "viz_paths = visualize_all_results(\n",
        "    results=test_results,\n",
        "    history_df=history_df,\n",
        "    output_dir=config['results_dir'],\n",
        "    timestamp=timestamp\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ VISUALIZACIONES GENERADAS\".center(70))\n",
        "print(\"=\"*70)\n",
        "for name, path in viz_paths.items():\n",
        "    print(f\"  {name:25s}: {path}\")\n",
        "print(\"=\"*70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8Ô∏è‚É£ Guardar y Descargar Resultados <a id=\"8\"></a>\n",
        "\n",
        "### Guardar Resultados Completos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from colab_utils.evaluation import save_results\n",
        "\n",
        "# Preparar informaci√≥n del checkpoint\n",
        "checkpoint_info = {\n",
        "    'best_epoch': int(checkpoint['epoch']),\n",
        "    'best_val_loss': float(checkpoint['val_loss']),\n",
        "    'best_val_acc': float(checkpoint['val_acc']),\n",
        "    'total_epochs_trained': len(training_history),\n",
        "}\n",
        "\n",
        "# Guardar todos los resultados\n",
        "json_path, txt_path, pred_path, ts = save_results(\n",
        "    results=test_results,\n",
        "    config=config,\n",
        "    checkpoint_info=checkpoint_info,\n",
        "    output_dir=config['results_dir']\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìÅ ARCHIVOS GENERADOS\".center(80))\n",
        "print(\"=\"*80)\n",
        "print(f\"Checkpoints:      {run_checkpoint_dir}\")\n",
        "print(f\"Logs TensorBoard: {log_dir}\")\n",
        "print(f\"JSON completo:    {json_path}\")\n",
        "print(f\"Reporte TXT:      {txt_path}\")\n",
        "print(f\"Predicciones CSV: {pred_path}\")\n",
        "print(f\"Visualizaciones:  {config['results_dir']}\")\n",
        "print(\"=\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Copiar Resultados a Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Crear carpetas en Drive si no existen\n",
        "drive_results = os.path.join(DRIVE_ROOT, \"results\")\n",
        "drive_checkpoints = os.path.join(DRIVE_ROOT, \"checkpoints\")\n",
        "drive_logs = os.path.join(DRIVE_ROOT, \"logs\")\n",
        "\n",
        "os.makedirs(drive_results, exist_ok=True)\n",
        "os.makedirs(drive_checkpoints, exist_ok=True)\n",
        "os.makedirs(drive_logs, exist_ok=True)\n",
        "\n",
        "print(\"\\nüì¶ Copiando resultados a Google Drive...\\n\")\n",
        "\n",
        "# Copiar resultados\n",
        "dest_results = os.path.join(drive_results, os.path.basename(config['results_dir']))\n",
        "if os.path.exists(dest_results):\n",
        "    shutil.rmtree(dest_results)\n",
        "shutil.copytree(config['results_dir'], dest_results)\n",
        "print(f\"‚úÖ Resultados:   {dest_results}\")\n",
        "\n",
        "# Copiar checkpoints\n",
        "dest_checkpoint = os.path.join(drive_checkpoints, os.path.basename(run_checkpoint_dir))\n",
        "if os.path.exists(dest_checkpoint):\n",
        "    shutil.rmtree(dest_checkpoint)\n",
        "shutil.copytree(run_checkpoint_dir, dest_checkpoint)\n",
        "print(f\"‚úÖ Checkpoints:  {dest_checkpoint}\")\n",
        "\n",
        "# Copiar logs\n",
        "dest_logs = os.path.join(drive_logs, os.path.basename(log_dir))\n",
        "if os.path.exists(dest_logs):\n",
        "    shutil.rmtree(dest_logs)\n",
        "shutil.copytree(log_dir, dest_logs)\n",
        "print(f\"‚úÖ Logs:         {dest_logs}\")\n",
        "\n",
        "print(\"\\n‚úÖ Todos los archivos copiados a Google Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Descargar Resultados Comprimidos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Crear nombre del archivo ZIP\n",
        "experiment_name = f\"{DATASET_TYPE}_{VERSION}_{timestamp}\"\n",
        "zip_name = f\"results_{experiment_name}.zip\"\n",
        "\n",
        "# Comprimir todo\n",
        "print(f\"\\nüì¶ Comprimiendo resultados...\\n\")\n",
        "!zip -r -q {zip_name} \\\n",
        "    {config['results_dir']} \\\n",
        "    {run_checkpoint_dir} \\\n",
        "    {log_dir}\n",
        "\n",
        "# Mostrar tama√±o\n",
        "zip_size_mb = os.path.getsize(zip_name) / (1024**2)\n",
        "print(f\"‚úÖ Archivo creado: {zip_name} ({zip_size_mb:.2f} MB)\")\n",
        "\n",
        "# Descargar\n",
        "print(f\"\\n‚¨áÔ∏è  Descargando...\")\n",
        "files.download(zip_name)\n",
        "\n",
        "print(\"\\n‚úÖ ¬°Descarga completada!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 9Ô∏è‚É£ Utilidades <a id=\"9\"></a>\n",
        "\n",
        "### Monitorear Uso de GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Limpiar Memoria GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "# Limpiar cache de GPU\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"‚úÖ Memoria GPU liberada\")\n",
        "\n",
        "# Verificar memoria disponible\n",
        "if torch.cuda.is_available():\n",
        "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
        "    print(f\"\\nMemoria GPU:\")\n",
        "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
        "    print(f\"  Reserved:  {reserved:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ver Historial de Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Mostrar tabla de historial\n",
        "history_df = pd.DataFrame(training_history)\n",
        "print(\"\\nüìä HISTORIAL DE ENTRENAMIENTO\\n\")\n",
        "print(history_df.to_string(index=False))\n",
        "\n",
        "# Mejores m√©tricas\n",
        "best_train_acc = history_df['train_acc'].max()\n",
        "best_val_acc = history_df['val_acc'].max()\n",
        "best_val_loss = history_df['val_loss'].min()\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Mejor Train Acc:  {best_train_acc:.2f}%\")\n",
        "print(f\"Mejor Val Acc:    {best_val_acc:.2f}%\")\n",
        "print(f\"Mejor Val Loss:   {best_val_loss:.4f}\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üîü Soluci√≥n de Problemas <a id=\"10\"></a>\n",
        "\n",
        "### ‚ùå Error: Out of Memory (OOM)\n",
        "\n",
        "**Soluciones:**\n",
        "1. Reduce el batch size:\n",
        "   ```python\n",
        "   config['batch_size'] = 4  # o incluso 2\n",
        "   ```\n",
        "2. Limpia la memoria GPU (ejecuta la celda de \"Limpiar Memoria GPU\")\n",
        "3. Reinicia el runtime: `Runtime > Restart runtime`\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ùå Error: No GPU available\n",
        "\n",
        "**Soluci√≥n:**\n",
        "1. Ve a `Runtime > Change runtime type`\n",
        "2. Selecciona `GPU` como Hardware accelerator\n",
        "3. Guarda y reconecta\n",
        "\n",
        "---\n",
        "\n",
        "### ‚è±Ô∏è Sesi√≥n de Colab se Desconecta\n",
        "\n",
        "**Informaci√≥n importante:**\n",
        "- Colab gratuito: m√°ximo 12 horas continuas\n",
        "- Los checkpoints se guardan autom√°ticamente cada N epochs\n",
        "- Usa Google Drive para respaldos autom√°ticos\n",
        "\n",
        "**Para resumir entrenamiento:**\n",
        "```python\n",
        "# (Funcionalidad en desarrollo)\n",
        "# config['resume_from'] = 'path/to/checkpoint.pt'\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üìÅ Videos No Se Cargan\n",
        "\n",
        "**Verificaciones:**\n",
        "1. Verifica que los videos est√©n en la carpeta correcta\n",
        "2. Ejecuta la celda de \"Verificar Estructura de Datasets\"\n",
        "3. Revisa los paths en la configuraci√≥n\n",
        "4. Aseg√∫rate que los videos sean `.mp4`\n",
        "\n",
        "---\n",
        "\n",
        "### üêõ Otros Problemas\n",
        "\n",
        "Si encuentras otros errores:\n",
        "1. Revisa los logs en la consola\n",
        "2. Verifica que todas las dependencias est√©n instaladas\n",
        "3. Reinicia el runtime y vuelve a ejecutar desde el principio\n",
        "4. Consulta la documentaci√≥n en el repositorio GitHub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ ¬°Experimento Completado!\n",
        "\n",
        "### üìä Resumen del Experimento\n",
        "\n",
        "- **Dataset:** {DATASET_TYPE.upper()}\n",
        "- **Versi√≥n:** {VERSION.upper()}\n",
        "- **Clases:** {config['num_classes']}\n",
        "- **Videos de Entrenamiento:** {len(train_dataset)}\n",
        "- **Test Accuracy:** Ver resultados en la secci√≥n 6\n",
        "\n",
        "### üìÅ Archivos Generados\n",
        "\n",
        "Todos los archivos est√°n guardados en:\n",
        "- **Google Drive:** `{DRIVE_ROOT}/`\n",
        "- **Colab Local:** `{config['results_dir']}`\n",
        "\n",
        "### üìö Pr√≥ximos Pasos\n",
        "\n",
        "1. Analiza las visualizaciones generadas\n",
        "2. Revisa el reporte TXT para tu tesis\n",
        "3. Compara con otros experimentos (V1 vs V2)\n",
        "4. Considera entrenar con el otro dataset (100 vs 300 clases)\n",
        "\n",
        "---\n",
        "\n",
        "**üéì Desarrollado por:** Rafael Ovalle - UNAB\n",
        "\n",
        "**üìß Contacto:** [tu-email@ejemplo.com]\n",
        "\n",
        "**üîó Repositorio:** https://github.com/Ov4llezz/AtiendeSenas-MVP\n",
        "\n",
        "---\n",
        "\n",
        "### ‚≠ê ¬°√âxito con tu Tesis!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
