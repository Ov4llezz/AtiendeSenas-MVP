{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMJl4kFwiUlZ"
   },
   "source": [
    "# ü§ñ VideoMAE - Reconocimiento de Lengua de Se√±as (WLASL)\n",
    "\n",
    "**Proyecto**: Reconocimiento de Lengua de Se√±as Americana con VideoMAE\n",
    "\n",
    "**Autor**: Rafael Ovalle - Tesis UNAB\n",
    "\n",
    "**Modelo**: VideoMAE (Video Masked Autoencoder) de Microsoft Research\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Configuraciones Disponibles\n",
    "\n",
    "| Config | Dataset | Train | Val | Test | Batch | LR | Regularizaci√≥n | Uso |\n",
    "|--------|---------|-------|-----|------|-------|----|--------------|---------|\n",
    "| **V1-100** | 100 clases | 807 | 194 | 117 | 16 | 1e-4 | Alta | Baseline |\n",
    "| **V2-100** | 100 clases | 1,001 | 117 | 117 | 6 | 1e-5 | Baja | Maximizar datos |\n",
    "| **V1-300** | 300 clases | 1,959 | 557 | 271 | 16 | 1e-4 | Alta | Baseline |\n",
    "| **V2-300** | 300 clases | 2,516 | 271 | 271 | 6 | 1e-5 | Baja | Maximizar datos |\n",
    "\n",
    "**Diferencias V1 vs V2:**\n",
    "- **V1**: Split tradicional (train/val/test separados) + regularizaci√≥n completa\n",
    "- **V2**: Train+Val combinados, Test como validaci√≥n + regularizaci√≥n reducida\n",
    "\n",
    "---\n",
    "\n",
    "## üìö √çndice\n",
    "1. [Configuraci√≥n Inicial](#1)\n",
    "2. [Verificar y Copiar Datasets](#2)\n",
    "3. [Configurar Experimento](#3)\n",
    "4. [Cargar Datos](#4)\n",
    "5. [Entrenamiento](#5)\n",
    "6. [Evaluaci√≥n](#6)\n",
    "7. [Visualizaciones](#7)\n",
    "8. [Guardar y Descargar Resultados](#8)\n",
    "9. [Utilidades](#9)\n",
    "10. [Soluci√≥n de Problemas](#10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6GZDsTHiUlb"
   },
   "source": [
    "---\n",
    "## 1Ô∏è‚É£ Configuraci√≥n Inicial <a id=\"1\"></a>\n",
    "\n",
    "### Verificar GPU y Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nh4DZ4HwiUlc"
   },
   "outputs": [],
   "source": [
    "# Verificar informaci√≥n del sistema\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"INFORMACI√ìN DEL SISTEMA\".center(70))\n",
    "print(\"=\"*70)\n",
    "print(f\"Python:  {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA:    {torch.version.cuda if torch.cuda.is_available() else 'No disponible'}\")\n",
    "print(f\"\\nGPU disponible: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"Memoria GPU: {gpu_memory_gb:.2f} GB\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: GPU no disponible!\")\n",
    "    print(\"   El entrenamiento ser√° MUY lento.\")\n",
    "    print(\"   Soluci√≥n: Runtime > Change runtime type > GPU\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8i6yP0zSiUld"
   },
   "outputs": [],
   "source": [
    "# Verificar GPU con nvidia-smi\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_sS2KiziUlg"
   },
   "source": [
    "### Clonar Repositorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9mfDZ7nxiUlh"
   },
   "outputs": [],
   "source": "import os\n\n# Configurar Git (opcional, para commits)\n!git config --global user.email \"ov4lle23@gmial.com\"\n!git config --global user.name \"ov4llezz\"\n\n# Clonar repositorio si no existe\nif not os.path.exists('AtiendeSenas-MVP'):\n    print(\"[INFO] Clonando repositorio...\")\n    !git clone https://github.com/Ov4llezz/AtiendeSenas-MVP.git\n    %cd AtiendeSenas-MVP\nelse:\n    print(\"[INFO] Repositorio ya existe, actualizando...\")\n    %cd AtiendeSenas-MVP\n    !git pull\n\nprint(\"\\n‚úÖ Repositorio listo\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MX3785hciUld"
   },
   "source": [
    "### Clonar dataset de Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoO-PqjJiUle"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# 1. INSTALAR UNZIP (Esto es lo que faltaba)\n",
    "print(\"üîß Instalando herramienta unzip...\")\n",
    "!sudo apt-get update > /dev/null\n",
    "!sudo apt-get install -y unzip > /dev/null\n",
    "\n",
    "\n",
    "# 2. Configurar rutas\n",
    "destination_folder = \"/home/ov4lle/AtiendeSenas-MVP\"\n",
    "zip_filename = \"dataset_master.zip\"\n",
    "os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "# 3. ID del archivo (El mismo de antes)\n",
    "file_id = '1vR_PHJ2myA933uzbY_Dh5rrVwffTZ3mC'\n",
    "\n",
    "# 4. Descargar de nuevo (porque el anterior se borr√≥)\n",
    "print(f\"‚¨áÔ∏è Descargando de nuevo...\")\n",
    "url = f'https://drive.google.com/uc?id={file_id}'\n",
    "output_path = f\"{destination_folder}/{zip_filename}\"\n",
    "\n",
    "# Usamos gdown\n",
    "!pip install -q gdown\n",
    "!gdown {url} -O \"{output_path}\"\n",
    "\n",
    "# 5. Descomprimir (Ahora s√≠ funcionar√°)\n",
    "print(f\"üì¶ Descomprimiendo...\")\n",
    "if os.path.exists(output_path):\n",
    "    !unzip -q -o \"{output_path}\" -d \"{destination_folder}\"\n",
    "\n",
    "    # 6. Borrar el zip\n",
    "    !rm \"{output_path}\"\n",
    "    print(\"‚úÖ ¬°Listo! Dataset descomprimido correctamente.\")\n",
    "else:\n",
    "    print(\"‚ùå Error: El archivo zip no se descarg√≥.\")\n",
    "\n",
    "# Verificar archivos\n",
    "!ls -lh \"{destination_folder}\" | head -n 10\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OefkzkLQiUli"
   },
   "source": [
    "### Instalar Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dk1_q9qXiUlj"
   },
   "outputs": [],
   "source": [
    "# Instalar todas las dependencias necesarias\n",
    "from colab_utils.config import setup_environment\n",
    "\n",
    "print(\"[INFO] Instalando dependencias...\\n\")\n",
    "setup_environment()\n",
    "print(\"\\n‚úÖ Todas las dependencias instaladas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufoD0QBFiUlk"
   },
   "source": [
    "---\n",
    "## 2Ô∏è‚É£ Verificar  Datasets <a id=\"2\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbCwAiHCiUll"
   },
   "source": [
    "### Verificar Estructura de Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5LTD0rWiUll"
   },
   "outputs": [],
   "source": "import os\n\ndef verificar_dataset(dataset_path, dataset_name):\n    \"\"\"Verifica estructura y cuenta videos en cada split\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"DATASET: {dataset_name}\".center(60))\n    print(\"=\"*60)\n    \n    if not os.path.exists(dataset_path):\n        print(f\"‚ùå No encontrado: {dataset_path}\")\n        return\n    \n    splits = ['train', 'val', 'test']\n    total_videos = 0\n    \n    for split in splits:\n        split_path = os.path.join(dataset_path, 'dataset', split)\n        \n        if os.path.exists(split_path):\n            videos = [f for f in os.listdir(split_path) if f.endswith('.mp4')]\n            num_videos = len(videos)\n            total_videos += num_videos\n            print(f\"{split.capitalize():5s}: {num_videos:5d} videos\")\n        else:\n            print(f\"{split.capitalize():5s}: ‚ùå No existe\")\n    \n    print(f\"{'‚îÄ'*60}\")\n    print(f\"TOTAL: {total_videos:5d} videos\")\n    print(\"=\"*60)\n\n# Ruta base de datos en la VM\nDATA_ROOT = \"/home/ov4lle/AtiendeSenas-MVP/data\"\n\n# Verificar los 4 datasets\nprint(\"\\nüîç VERIFICANDO TODOS LOS DATASETS\")\nverificar_dataset(f'{DATA_ROOT}/wlasl100', 'WLASL100 (V1)')\nverificar_dataset(f'{DATA_ROOT}/wlasl300', 'WLASL300 (V1)')\nverificar_dataset(f'{DATA_ROOT}/wlasl100_v2', 'WLASL100 (V2 - Train+Val combinados)')\nverificar_dataset(f'{DATA_ROOT}/wlasl300_v2', 'WLASL300 (V2 - Train+Val combinados)')"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Modf4xdziUll"
   },
   "source": [
    "---\n",
    "## 3Ô∏è‚É£ Configurar Experimento <a id=\"3\"></a>\n",
    "\n",
    "### üéØ Selecciona tu configuraci√≥n de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDrpXEoxiUlm"
   },
   "outputs": [],
   "source": "from colab_utils.config import create_config, print_config, save_config\n\n# ============================================================\n#   üéØ CONFIGURA TU EXPERIMENTO AQU√ç\n# ============================================================\n\n# Ruta base de datos (VM)\nDATA_ROOT = \"/home/ov4lle/AtiendeSenas-MVP/data\"\n\n# Selecci√≥n de Dataset y Versi√≥n\nDATASET_TYPE = \"wlasl100\"  # Opciones: \"wlasl100\" o \"wlasl300\"\nVERSION = \"v1\"             # Opciones: \"v1\" o \"v2\"\n\n# === HIPERPAR√ÅMETROS (Modificables, valores por defecto mostrados) ===\nBATCH_SIZE = 6              # Tama√±o del batch\nMAX_EPOCHS = 30             # N√∫mero m√°ximo de epochs\nLEARNING_RATE = 1e-5        # Learning rate inicial\nPATIENCE = 10               # Patience para early stopping\nWEIGHT_DECAY = 0.0          # Regularizaci√≥n L2 (0.0 = desactivado)\nLABEL_SMOOTHING = 0.0       # Label smoothing (0.0 = desactivado)\nCLASS_WEIGHTED = False      # Usar pesos de clases en la p√©rdida\nFREEZE_BACKBONE = False     # True = solo entrenar clasificador, False = entrenar todo\n\n# === OTROS PAR√ÅMETROS (Avanzados) ===\nWARMUP_RATIO = 0.1          # Porcentaje de warmup\nMIN_LR = 1e-6               # Learning rate m√≠nimo\nGRADIENT_CLIP = 1.0         # Gradient clipping\nNUM_WORKERS = 2             # Workers para DataLoader\nSAVE_EVERY = 5              # Guardar checkpoint cada N epochs\n\n# ============================================================\n\n# Crear configuraci√≥n\nconfig = create_config(\n    dataset_type=DATASET_TYPE,\n    version=VERSION,\n    data_root=DATA_ROOT,\n    batch_size=BATCH_SIZE,\n    max_epochs=MAX_EPOCHS,\n    learning_rate=LEARNING_RATE,\n    patience=PATIENCE,\n    weight_decay=WEIGHT_DECAY,\n    label_smoothing=LABEL_SMOOTHING,\n    class_weighted=CLASS_WEIGHTED,\n    freeze_backbone=FREEZE_BACKBONE,\n    warmup_ratio=WARMUP_RATIO,\n    min_lr=MIN_LR,\n    gradient_clip=GRADIENT_CLIP,\n    num_workers=NUM_WORKERS,\n    save_every=SAVE_EVERY\n)\n\n# Mostrar configuraci√≥n\nprint_config(config)\n\n# Guardar configuraci√≥n\nsave_config(config, config['results_dir'])\n\nprint(f\"\\n‚úÖ Configuraci√≥n creada: {DATASET_TYPE.upper()} {VERSION.upper()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljwogpwXiUlm"
   },
   "source": [
    "### ‚öôÔ∏è Ajustes Manuales (Opcional)\n",
    "\n",
    "Si quieres personalizar hiperpar√°metros espec√≠ficos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZMVNSv9iUln"
   },
   "source": [
    "---\n",
    "## 4Ô∏è‚É£ Cargar Datos <a id=\"4\"></a>\n",
    "\n",
    "### Crear Datasets y DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzzsNqxZiUln"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from colab_utils.dataset import WLASLVideoDataset\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CARGANDO DATASETS\".center(70))\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Crear datasets\n",
    "print(\"[1/3] Creando Train Dataset...\")\n",
    "train_dataset = WLASLVideoDataset(\n",
    "    split=\"train\",\n",
    "    base_path=config['data_root'],\n",
    "    dataset_size=config['num_classes']\n",
    ")\n",
    "\n",
    "print(\"[2/3] Creando Validation Dataset...\")\n",
    "val_dataset = WLASLVideoDataset(\n",
    "    split=\"val\",\n",
    "    base_path=config['data_root'],\n",
    "    dataset_size=config['num_classes']\n",
    ")\n",
    "\n",
    "print(\"[3/3] Creando Test Dataset...\")\n",
    "test_dataset = WLASLVideoDataset(\n",
    "    split=\"test\",\n",
    "    base_path=config['data_root'],\n",
    "    dataset_size=config['num_classes']\n",
    ")\n",
    "\n",
    "# Crear dataloaders\n",
    "print(\"\\n[INFO] Creando DataLoaders...\")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=config['num_workers'],\n",
    "    pin_memory=True if config['device'] == \"cuda\" else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=config['num_workers'],\n",
    "    pin_memory=True if config['device'] == \"cuda\" else False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=config['num_workers'],\n",
    "    pin_memory=True if config['device'] == \"cuda\" else False\n",
    ")\n",
    "\n",
    "# Resumen\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATASETS CARGADOS EXITOSAMENTE\".center(70))\n",
    "print(\"=\"*70)\n",
    "print(f\"Train:      {len(train_dataset):>5,} videos ({len(train_loader):>4} batches)\")\n",
    "print(f\"Validation: {len(val_dataset):>5,} videos ({len(val_loader):>4} batches)\")\n",
    "print(f\"Test:       {len(test_dataset):>5,} videos ({len(test_loader):>4} batches)\")\n",
    "print(f\"\\nClases:     {config['num_classes']}\")\n",
    "print(f\"Batch size: {config['batch_size']}\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVSkzlDciUln"
   },
   "source": [
    "---\n",
    "## 5Ô∏è‚É£ Entrenamiento <a id=\"5\"></a>\n",
    "\n",
    "### Iniciar Entrenamiento Completo\n",
    "\n",
    "‚è∞ **Tiempo estimado:**\n",
    "- WLASL100 V1: ~2-3 horas (GPU T4)\n",
    "- WLASL100 V2: ~3-4 horas (GPU T4)\n",
    "- WLASL300 V1: ~6-8 horas (GPU T4)\n",
    "- WLASL300 V2: ~8-12 horas (GPU T4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74nSC4tPiUlo"
   },
   "outputs": [],
   "source": [
    "from colab_utils.training import train_model\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ INICIANDO ENTRENAMIENTO\".center(70))\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Entrenar modelo\n",
    "model, training_history, run_checkpoint_dir, log_dir = train_model(\n",
    "    config=config,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    train_dataset=train_dataset\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ ENTRENAMIENTO COMPLETADO\".center(70))\n",
    "print(\"=\"*70)\n",
    "print(f\"Checkpoints: {run_checkpoint_dir}\")\n",
    "print(f\"Logs:        {log_dir}\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vg3dxUMfiUlo"
   },
   "source": [
    "### üéØ Entrenamiento R√°pido (Solo para Pruebas)\n",
    "\n",
    "Si solo quieres verificar que todo funciona correctamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20NGLHw6iUlo"
   },
   "outputs": [],
   "source": [
    "#PRUEBA R√ÅPIDA (1-2 epochs)\n",
    "\n",
    "config_test = config.copy()\n",
    "config_test['max_epochs'] = 2\n",
    "config_test['patience'] = 5\n",
    "\n",
    "print(\"‚ö° ENTRENAMIENTO DE PRUEBA (2 epochs)\\n\")\n",
    "model, training_history, run_checkpoint_dir, log_dir = train_model(\n",
    "     config=config_test,\n",
    "     train_loader=train_loader,\n",
    "     val_loader=val_loader,\n",
    "     train_dataset=train_dataset\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-pk60_kiUlo"
   },
   "source": [
    "### Visualizar TensorBoard\n",
    "\n",
    "Monitorea el entrenamiento en tiempo real:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KxkIK3bHiUlo"
   },
   "outputs": [],
   "source": [
    "# Cargar extensi√≥n TensorBoard\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Lanzar TensorBoard\n",
    "%tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Bo7z6wKiUlp"
   },
   "source": [
    "---\n",
    "## 6Ô∏è‚É£ Evaluaci√≥n <a id=\"6\"></a>\n",
    "\n",
    "### Cargar Mejor Modelo y Evaluar en Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dkrq8YANiUlp"
   },
   "outputs": [],
   "source": [
    "from colab_utils.evaluation import evaluate_detailed, print_results, print_top_classes\n",
    "import torch\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CARGANDO MEJOR MODELO\".center(70))\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Cargar mejor modelo\n",
    "best_model_path = f\"{run_checkpoint_dir}/best_model.pt\"\n",
    "checkpoint = torch.load(best_model_path, map_location=config['device'])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"‚úÖ Modelo cargado exitosamente\")\n",
    "print(f\"   Epoch:    {checkpoint['epoch']}\")\n",
    "print(f\"   Val Loss: {checkpoint['val_loss']:.4f}\")\n",
    "print(f\"   Val Acc:  {checkpoint['val_acc']:.2f}%\")\n",
    "\n",
    "# Evaluar en test set\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUACI√ìN EN TEST SET\".center(70))\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "test_results = evaluate_detailed(\n",
    "    model=model,\n",
    "    dataloader=test_loader,\n",
    "    device=config['device'],\n",
    "    num_classes=config['num_classes']\n",
    ")\n",
    "\n",
    "# Mostrar resultados generales\n",
    "print_results(test_results)\n",
    "\n",
    "# Mostrar mejores y peores clases\n",
    "print_top_classes(test_results, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwtiGgVpiUlp"
   },
   "source": [
    "---\n",
    "## 7Ô∏è‚É£ Visualizaciones <a id=\"7\"></a>\n",
    "\n",
    "### Curvas de Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvOba_LZiUlp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from colab_utils.visualization import plot_training_curves\n",
    "from datetime import datetime\n",
    "\n",
    "# Preparar datos\n",
    "history_df = pd.DataFrame(training_history)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Graficar curvas de entrenamiento\n",
    "curves_path = f\"{config['results_dir']}/training_curves_{timestamp}.png\"\n",
    "plot_training_curves(history_df, save_path=curves_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRlt7M7iiUlp"
   },
   "source": [
    "### Todas las Visualizaciones\n",
    "\n",
    "Genera todas las visualizaciones: matriz de confusi√≥n, performance por clase, distribuciones, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvklaZh3iUlq"
   },
   "outputs": [],
   "source": [
    "from colab_utils.visualization import visualize_all_results\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERANDO VISUALIZACIONES\".center(70))\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Generar todas las visualizaciones\n",
    "viz_paths = visualize_all_results(\n",
    "    results=test_results,\n",
    "    history_df=history_df,\n",
    "    output_dir=config['results_dir'],\n",
    "    timestamp=timestamp\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ VISUALIZACIONES GENERADAS\".center(70))\n",
    "print(\"=\"*70)\n",
    "for name, path in viz_paths.items():\n",
    "    print(f\"  {name:25s}: {path}\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jY5qtzP4iUlq"
   },
   "source": [
    "---\n",
    "## 8Ô∏è‚É£ Guardar y Descargar Resultados <a id=\"8\"></a>\n",
    "\n",
    "### Guardar Resultados Completos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftQtv7wNiUlq"
   },
   "outputs": [],
   "source": [
    "from colab_utils.evaluation import save_results\n",
    "\n",
    "# Preparar informaci√≥n del checkpoint\n",
    "checkpoint_info = {\n",
    "    'best_epoch': int(checkpoint['epoch']),\n",
    "    'best_val_loss': float(checkpoint['val_loss']),\n",
    "    'best_val_acc': float(checkpoint['val_acc']),\n",
    "    'total_epochs_trained': len(training_history),\n",
    "}\n",
    "\n",
    "# Guardar todos los resultados\n",
    "json_path, txt_path, pred_path, ts = save_results(\n",
    "    results=test_results,\n",
    "    config=config,\n",
    "    checkpoint_info=checkpoint_info,\n",
    "    output_dir=config['results_dir']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìÅ ARCHIVOS GENERADOS\".center(80))\n",
    "print(\"=\"*80)\n",
    "print(f\"Checkpoints:      {run_checkpoint_dir}\")\n",
    "print(f\"Logs TensorBoard: {log_dir}\")\n",
    "print(f\"JSON completo:    {json_path}\")\n",
    "print(f\"Reporte TXT:      {txt_path}\")\n",
    "print(f\"Predicciones CSV: {pred_path}\")\n",
    "print(f\"Visualizaciones:  {config['results_dir']}\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yRHknuHiUlr"
   },
   "source": [
    "### Copiar Resultados a Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvtr9F2oiUlr"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Crear carpetas en Drive si no existen\n",
    "drive_results = os.path.join(DRIVE_ROOT, \"results\")\n",
    "drive_checkpoints = os.path.join(DRIVE_ROOT, \"checkpoints\")\n",
    "drive_logs = os.path.join(DRIVE_ROOT, \"logs\")\n",
    "\n",
    "os.makedirs(drive_results, exist_ok=True)\n",
    "os.makedirs(drive_checkpoints, exist_ok=True)\n",
    "os.makedirs(drive_logs, exist_ok=True)\n",
    "\n",
    "print(\"\\nüì¶ Copiando resultados a Google Drive...\\n\")\n",
    "\n",
    "# Copiar resultados\n",
    "dest_results = os.path.join(drive_results, os.path.basename(config['results_dir']))\n",
    "if os.path.exists(dest_results):\n",
    "    shutil.rmtree(dest_results)\n",
    "shutil.copytree(config['results_dir'], dest_results)\n",
    "print(f\"‚úÖ Resultados:   {dest_results}\")\n",
    "\n",
    "# Copiar checkpoints\n",
    "dest_checkpoint = os.path.join(drive_checkpoints, os.path.basename(run_checkpoint_dir))\n",
    "if os.path.exists(dest_checkpoint):\n",
    "    shutil.rmtree(dest_checkpoint)\n",
    "shutil.copytree(run_checkpoint_dir, dest_checkpoint)\n",
    "print(f\"‚úÖ Checkpoints:  {dest_checkpoint}\")\n",
    "\n",
    "# Copiar logs\n",
    "dest_logs = os.path.join(drive_logs, os.path.basename(log_dir))\n",
    "if os.path.exists(dest_logs):\n",
    "    shutil.rmtree(dest_logs)\n",
    "shutil.copytree(log_dir, dest_logs)\n",
    "print(f\"‚úÖ Logs:         {dest_logs}\")\n",
    "\n",
    "print(\"\\n‚úÖ Todos los archivos copiados a Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUnhG4AziUlr"
   },
   "source": "# NOTA: Esta celda es para copiar resultados a Google Drive\n# En la VM, los resultados ya est√°n en disco local\n# Si quieres hacer backup, puedes usar rsync, scp, o similar\n\n# import shutil\n# import os\n\n# # Crear carpetas en Drive si no existen\n# drive_results = \"/ruta/a/tu/backup/results\"\n# drive_checkpoints = \"/ruta/a/tu/backup/checkpoints\"\n# drive_logs = \"/ruta/a/tu/backup/logs\"\n\n# os.makedirs(drive_results, exist_ok=True)\n# os.makedirs(drive_checkpoints, exist_ok=True)\n# os.makedirs(drive_logs, exist_ok=True)\n\n# print(\"\\nüì¶ Copiando resultados...\\n\")\n\n# # Copiar resultados\n# dest_results = os.path.join(drive_results, os.path.basename(config['results_dir']))\n# if os.path.exists(dest_results):\n#     shutil.rmtree(dest_results)\n# shutil.copytree(config['results_dir'], dest_results)\n# print(f\"‚úÖ Resultados:   {dest_results}\")\n\n# # Copiar checkpoints\n# dest_checkpoint = os.path.join(drive_checkpoints, os.path.basename(run_checkpoint_dir))\n# if os.path.exists(dest_checkpoint):\n#     shutil.rmtree(dest_checkpoint)\n# shutil.copytree(run_checkpoint_dir, dest_checkpoint)\n# print(f\"‚úÖ Checkpoints:  {dest_checkpoint}\")\n\n# # Copiar logs\n# dest_logs = os.path.join(drive_logs, os.path.basename(log_dir))\n# if os.path.exists(dest_logs):\n#     shutil.rmtree(dest_logs)\n# shutil.copytree(log_dir, dest_logs)\n# print(f\"‚úÖ Logs:         {dest_logs}\")\n\n# print(\"\\n‚úÖ Todos los archivos copiados\")\n\nprint(\"‚ÑπÔ∏è  Los resultados est√°n guardados en:\")\nprint(f\"  - Checkpoints: {run_checkpoint_dir}\")\nprint(f\"  - Logs: {log_dir}\")\nprint(f\"  - Results: {config['results_dir']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ji-lN_m4iUlr"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Crear nombre del archivo ZIP\n",
    "experiment_name = f\"{DATASET_TYPE}_{VERSION}_{timestamp}\"\n",
    "zip_name = f\"results_{experiment_name}.zip\"\n",
    "\n",
    "# Comprimir todo\n",
    "print(f\"\\nüì¶ Comprimiendo resultados...\\n\")\n",
    "!zip -r -q {zip_name} \\\n",
    "    {config['results_dir']} \\\n",
    "    {run_checkpoint_dir} \\\n",
    "    {log_dir}\n",
    "\n",
    "# Mostrar tama√±o\n",
    "zip_size_mb = os.path.getsize(zip_name) / (1024**2)\n",
    "print(f\"‚úÖ Archivo creado: {zip_name} ({zip_size_mb:.2f} MB)\")\n",
    "\n",
    "# Descargar\n",
    "print(f\"\\n‚¨áÔ∏è  Descargando...\")\n",
    "files.download(zip_name)\n",
    "\n",
    "print(\"\\n‚úÖ ¬°Descarga completada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Stfx-4viUls"
   },
   "source": "# NOTA: Esta celda es para descargar resultados desde Google Colab\n# En la VM, puedes acceder directamente a los archivos o usar scp/rsync\n\n# from google.colab import files\n# import os\n\n# # Crear nombre del archivo ZIP\n# experiment_name = f\"{DATASET_TYPE}_{VERSION}_{timestamp}\"\n# zip_name = f\"results_{experiment_name}.zip\"\n\n# # Comprimir todo\n# print(f\"\\nüì¶ Comprimiendo resultados...\\n\")\n# !zip -r -q {zip_name} \\\n#     {config['results_dir']} \\\n#     {run_checkpoint_dir} \\\n#     {log_dir}\n\n# # Mostrar tama√±o\n# zip_size_mb = os.path.getsize(zip_name) / (1024**2)\n# print(f\"‚úÖ Archivo creado: {zip_name} ({zip_size_mb:.2f} MB)\")\n\n# # Descargar\n# print(f\"\\n‚¨áÔ∏è  Descargando...\")\n# files.download(zip_name)\n\n# print(\"\\n‚úÖ ¬°Descarga completada!\")\n\nprint(\"‚ÑπÔ∏è  Para descargar resultados desde la VM, usa:\")\nprint(f\"  scp -r usuario@vm:/home/ov4lle/AtiendeSenas-MVP/results ./\")\nprint(f\"  scp -r usuario@vm:/home/ov4lle/AtiendeSenas-MVP/models ./\")\nprint(f\"  scp -r usuario@vm:/home/ov4lle/AtiendeSenas-MVP/runs ./\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7LILoMziUlw"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJ0jcjQviUlw"
   },
   "source": [
    "### Limpiar Memoria GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CUZtG-jIiUlx"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Limpiar cache de GPU\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"‚úÖ Memoria GPU liberada\")\n",
    "\n",
    "# Verificar memoria disponible\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    print(f\"\\nMemoria GPU:\")\n",
    "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"  Reserved:  {reserved:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfHU0WUsiUlx"
   },
   "source": [
    "### Ver Historial de Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-9qWi2giUlx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Mostrar tabla de historial\n",
    "history_df = pd.DataFrame(training_history)\n",
    "print(\"\\nüìä HISTORIAL DE ENTRENAMIENTO\\n\")\n",
    "print(history_df.to_string(index=False))\n",
    "\n",
    "# Mejores m√©tricas\n",
    "best_train_acc = history_df['train_acc'].max()\n",
    "best_val_acc = history_df['val_acc'].max()\n",
    "best_val_loss = history_df['val_loss'].min()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Mejor Train Acc:  {best_train_acc:.2f}%\")\n",
    "print(f\"Mejor Val Acc:    {best_val_acc:.2f}%\")\n",
    "print(f\"Mejor Val Loss:   {best_val_loss:.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CR_EMNUXiUlx"
   },
   "source": [
    "---\n",
    "## üîü Soluci√≥n de Problemas <a id=\"10\"></a>\n",
    "\n",
    "### ‚ùå Error: Out of Memory (OOM)\n",
    "\n",
    "**Soluciones:**\n",
    "1. Reduce el batch size:\n",
    "   ```python\n",
    "   config['batch_size'] = 4  # o incluso 2\n",
    "   ```\n",
    "2. Limpia la memoria GPU (ejecuta la celda de \"Limpiar Memoria GPU\")\n",
    "3. Reinicia el runtime: `Runtime > Restart runtime`\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Error: No GPU available\n",
    "\n",
    "**Soluci√≥n:**\n",
    "1. Ve a `Runtime > Change runtime type`\n",
    "2. Selecciona `GPU` como Hardware accelerator\n",
    "3. Guarda y reconecta\n",
    "\n",
    "---\n",
    "\n",
    "### ‚è±Ô∏è Sesi√≥n de Colab se Desconecta\n",
    "\n",
    "**Informaci√≥n importante:**\n",
    "- Colab gratuito: m√°ximo 12 horas continuas\n",
    "- Los checkpoints se guardan autom√°ticamente cada N epochs\n",
    "- Usa Google Drive para respaldos autom√°ticos\n",
    "\n",
    "**Para resumir entrenamiento:**\n",
    "```python\n",
    "# (Funcionalidad en desarrollo)\n",
    "# config['resume_from'] = 'path/to/checkpoint.pt'\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìÅ Videos No Se Cargan\n",
    "\n",
    "**Verificaciones:**\n",
    "1. Verifica que los videos est√©n en la carpeta correcta\n",
    "2. Ejecuta la celda de \"Verificar Estructura de Datasets\"\n",
    "3. Revisa los paths en la configuraci√≥n\n",
    "4. Aseg√∫rate que los videos sean `.mp4`\n",
    "\n",
    "---\n",
    "\n",
    "### üêõ Otros Problemas\n",
    "\n",
    "Si encuentras otros errores:\n",
    "1. Revisa los logs en la consola\n",
    "2. Verifica que todas las dependencias est√©n instaladas\n",
    "3. Reinicia el runtime y vuelve a ejecutar desde el principio\n",
    "4. Consulta la documentaci√≥n en el repositorio GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9jbmLYviUlx"
   },
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ ¬°Experimento Completado!\n",
    "\n",
    "### üìä Resumen del Experimento\n",
    "\n",
    "- **Dataset:** {DATASET_TYPE.upper()}\n",
    "- **Versi√≥n:** {VERSION.upper()}\n",
    "- **Clases:** {config['num_classes']}\n",
    "- **Videos de Entrenamiento:** {len(train_dataset)}\n",
    "- **Test Accuracy:** Ver resultados en la secci√≥n 6\n",
    "\n",
    "### üìÅ Archivos Generados\n",
    "\n",
    "Todos los archivos est√°n guardados en:\n",
    "- **Google Drive:** `{DRIVE_ROOT}/`\n",
    "- **Colab Local:** `{config['results_dir']}`\n",
    "\n",
    "### üìö Pr√≥ximos Pasos\n",
    "\n",
    "1. Analiza las visualizaciones generadas\n",
    "2. Revisa el reporte TXT para tu tesis\n",
    "3. Compara con otros experimentos (V1 vs V2)\n",
    "4. Considera entrenar con el otro dataset (100 vs 300 clases)\n",
    "\n",
    "---\n",
    "\n",
    "**üéì Desarrollado por:** Rafael Ovalle - UNAB\n",
    "\n",
    "**üìß Contacto:** [tu-email@ejemplo.com]\n",
    "\n",
    "**üîó Repositorio:** https://github.com/Ov4llezz/AtiendeSenas-MVP\n",
    "\n",
    "---\n",
    "\n",
    "### ‚≠ê ¬°√âxito con tu Tesis!"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## ‚úÖ ¬°Experimento Completado!\n\n### üìä Resumen del Experimento\n\nRevisa los resultados de tu entrenamiento:\n\n- **Dataset:** Ver variable `DATASET_TYPE`\n- **Versi√≥n:** Ver variable `VERSION`\n- **N√∫mero de Clases:** Ver `config['num_classes']`\n- **Videos de Entrenamiento:** Ver output de secci√≥n 4\n- **Test Accuracy:** Ver resultados en la secci√≥n 6\n\n### üìÅ Archivos Generados\n\nTodos los archivos est√°n guardados en la VM:\n\n- **Checkpoints:** `models/{VERSION}/{DATASET}/checkpoints/`\n- **Logs TensorBoard:** `runs/{VERSION}/{DATASET}/`\n- **Resultados:** `results/{VERSION}/{DATASET}/`\n\n### üìö Pr√≥ximos Pasos\n\n1. Analiza las visualizaciones generadas (secci√≥n 7)\n2. Revisa el reporte TXT para tu tesis\n3. Compara con otros experimentos (V1 vs V2)\n4. Considera entrenar con el otro dataset (100 vs 300 clases)\n5. Ajusta hiperpar√°metros y vuelve a entrenar si es necesario\n\n### üîß Hiperpar√°metros Utilizados\n\nLos hiperpar√°metros configurados en la secci√≥n 3 fueron:\n\n- **Batch Size:** Ver `BATCH_SIZE`\n- **Learning Rate:** Ver `LEARNING_RATE`\n- **Max Epochs:** Ver `MAX_EPOCHS`\n- **Patience:** Ver `PATIENCE`\n- **Weight Decay:** Ver `WEIGHT_DECAY`\n- **Label Smoothing:** Ver `LABEL_SMOOTHING`\n- **Class Weighted:** Ver `CLASS_WEIGHTED`\n- **Freeze Backbone:** Ver `FREEZE_BACKBONE`\n\n---\n\n**üéì Desarrollado por:** Rafael Ovalle - UNAB\n\n**üîó Repositorio:** https://github.com/Ov4llezz/AtiendeSenas-MVP\n\n---\n\n### ‚≠ê ¬°√âxito con tu Tesis!",
   "metadata": {
    "id": "2yDkHRQYtg1X"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}